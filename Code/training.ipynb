{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-17T12:37:44.698555Z",
     "start_time": "2020-04-17T12:37:37.412044Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\91828\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\91828\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\91828\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\91828\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\91828\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\91828\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "C:\\Users\\91828\\Anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\91828\\Anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\91828\\Anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\91828\\Anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\91828\\Anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\91828\\Anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "from datetime import datetime\n",
    "import os.path\n",
    "import time\n",
    "import numpy as np\n",
    "import librosa\n",
    "import pickle\n",
    "from numpy.lib import stride_tricks\n",
    "import ipdb\n",
    "import os\n",
    "import itertools\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-17T12:37:44.709501Z",
     "start_time": "2020-04-17T12:37:44.700558Z"
    }
   },
   "outputs": [],
   "source": [
    "FRAMES_PER_SAMPLE = 100  # number of frames forming a chunk of data\n",
    "SAMPLING_RATE = 8000\n",
    "FRAME_SIZE = 256\n",
    "NEFF = 129  # effective FFT points\n",
    "# amplification factor of the waveform sig\n",
    "AMP_FAC = 10000\n",
    "MIN_AMP = 10000\n",
    "# TF bins smaller than THRESHOLD will be\n",
    "# considered inactive\n",
    "THRESHOLD = 40\n",
    "# embedding dimention\n",
    "EMBBEDDING_D = 40\n",
    "# prams for pre-whitening\n",
    "GLOBAL_MEAN = 44\n",
    "GLOBAL_STD = 15.5\n",
    "# feed forward dropout prob\n",
    "P_DROPOUT_FF = 0.5\n",
    "# recurrent dropout prob\n",
    "P_DROPOUT_RC = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-17T12:37:44.757416Z",
     "start_time": "2020-04-17T12:37:44.711495Z"
    }
   },
   "outputs": [],
   "source": [
    "def stft(sig, frameSize, overlapFac=0.75, window=np.hanning):\n",
    "    \"\"\" short time fourier transform of audio signal \"\"\"\n",
    "    win = window(frameSize)\n",
    "    hopSize = int(frameSize - np.floor(overlapFac * frameSize))\n",
    "    samples = np.array(sig, dtype='float64')\n",
    "    # cols for windowing\n",
    "    cols = int(np.ceil((len(samples) - frameSize) / float(hopSize)))\n",
    "    frames = stride_tricks.as_strided(\n",
    "        samples,\n",
    "        shape=(cols, frameSize),\n",
    "        strides=(samples.strides[0] * hopSize, samples.strides[0])).copy()\n",
    "    frames *= win\n",
    "    return np.fft.rfft(frames)\n",
    "\n",
    "\n",
    "class DataGenerator(object):\n",
    "    def __init__(self, data_dir, batch_size):\n",
    "        '''preprocess the training data\n",
    "        data_dir: dir containing the training data\n",
    "                  format:root_dir + speaker_dir + wavfiles'''\n",
    "        # get dirs for each speaker\n",
    "        self.speakers_dir = [os.path.join(data_dir, i)\n",
    "                             for i in os.listdir(data_dir)]\n",
    "        self.n_speaker = len(self.speakers_dir)\n",
    "        self.batch_size = batch_size\n",
    "        self.speaker_file = {}\n",
    "        self.epoch = 0\n",
    "\n",
    "        # get the files in each speakers dir\n",
    "        for i in range(self.n_speaker):\n",
    "            wav_dir_i = [os.path.join(self.speakers_dir[i], file) for file in os.listdir(self.speakers_dir[i])]\n",
    "            for j in wav_dir_i:\n",
    "                if i not in self.speaker_file:\n",
    "                    self.speaker_file[i] = []\n",
    "                self.speaker_file[i].append(j)\n",
    "\n",
    "    def resample(self):\n",
    "        '''Resample all the files, not always necessary'''\n",
    "        for speaker in self.speaker_file:\n",
    "            for file in self.speaker_file[speaker]:\n",
    "                data, sr = librosa.load(file, SAMPLING_RATE)\n",
    "                librosa.output.write_wav(file, data, SAMPLING_RATE)\n",
    "\n",
    "    def reinit(self, save_path):\n",
    "        '''Init the training data using the wav files'''\n",
    "        self.speaker_file_match = {}\n",
    "        # generate match dict\n",
    "        for i in range(self.n_speaker):\n",
    "            for j in self.speaker_file[i]:\n",
    "                k = np.random.randint(self.n_speaker)\n",
    "                # requiring different speaker\n",
    "                while(i == k):\n",
    "                    k = np.random.randint(self.n_speaker)\n",
    "                l = np.random.randint(len(self.speaker_file[k]))\n",
    "                self.speaker_file_match[j] = self.speaker_file[k][l]\n",
    "\n",
    "        self.samples = []\n",
    "        self.ind = 0\n",
    "        # for each file pair, generate their mixture and reference samples\n",
    "        for i in self.speaker_file_match:\n",
    "            j = self.speaker_file_match[i]\n",
    "            speech_1, _ = librosa.core.load(i, sr=SAMPLING_RATE)\n",
    "            # amp factor between -3 dB - 3 dB\n",
    "            fac = np.random.rand(1)[0] * 6 - 3\n",
    "            speech_1 = 10. ** (fac / 20) * speech_1\n",
    "            speech_2, _ = librosa.core.load(j, sr=SAMPLING_RATE)\n",
    "            fac = np.random.rand(1)[0] * 6 - 3\n",
    "            speech_2 = 10. ** (fac / 20) * speech_2\n",
    "            # mix\n",
    "            length = min(len(speech_1), len(speech_2))\n",
    "            speech_1 = speech_1[:length]\n",
    "            speech_2 = speech_2[:length]\n",
    "            speech_mix = speech_1 + speech_2\n",
    "            # compute log spectrum for 1st speaker\n",
    "            speech_1_spec = np.abs(stft(speech_1, FRAME_SIZE)[:, :NEFF])\n",
    "            speech_1_spec = np.maximum(speech_1_spec, np.max(speech_1_spec) / MIN_AMP)\n",
    "            speech_1_spec = 20. * np.log10(speech_1_spec * AMP_FAC)\n",
    "            # same for the 2nd speaker\n",
    "            speech_2_spec = np.abs(stft(speech_2, FRAME_SIZE)[:, :NEFF])\n",
    "            speech_2_spec = np.maximum(speech_2_spec, np.max(speech_2_spec) / MIN_AMP)\n",
    "            speech_2_spec = 20. * np.log10(speech_2_spec * AMP_FAC)\n",
    "            # same for the mixture\n",
    "            speech_mix_spec0 = stft(speech_mix, FRAME_SIZE)[:, :NEFF]\n",
    "            speech_mix_spec = np.abs(speech_mix_spec0)\n",
    "            speech_mix_spec = np.maximum(speech_mix_spec, np.max(speech_mix_spec) / MIN_AMP)\n",
    "            speech_mix_spec = 20. * np.log10(speech_mix_spec * AMP_FAC)\n",
    "            \n",
    "            max_mag = np.max(speech_mix_spec)\n",
    "            speech_VAD = (speech_mix_spec > (max_mag - THRESHOLD)).astype(int)\n",
    "            speech_mix_spec = (speech_mix_spec - GLOBAL_MEAN) / GLOBAL_STD\n",
    "\n",
    "            len_spec = speech_1_spec.shape[0]\n",
    "            k = 0\n",
    "            while(k + FRAMES_PER_SAMPLE < len_spec):\n",
    "                sample_1 = speech_1_spec[k:k + FRAMES_PER_SAMPLE, :]\n",
    "                sample_2 = speech_2_spec[k:k + FRAMES_PER_SAMPLE, :]\n",
    "                sample_mix = speech_mix_spec[k:k + FRAMES_PER_SAMPLE, :].astype('float16')\n",
    "                # Y: indicator of the belongings of the TF bin\n",
    "                # 1st speaker or second speaker\n",
    "                Y = np.array([sample_1 > sample_2, sample_1 < sample_2]).astype('bool')\n",
    "                Y = np.transpose(Y, [1, 2, 0])\n",
    "                VAD = speech_VAD[k:k + FRAMES_PER_SAMPLE, :].astype('bool')\n",
    "                sample_dict = {'Sample': sample_mix,\n",
    "                               'VAD': VAD,\n",
    "                               'Target': Y}\n",
    "                self.samples.append(sample_dict)\n",
    "                k = k + FRAMES_PER_SAMPLE\n",
    "\n",
    "        pickle.dump(self.samples, open(save_path, 'wb'))\n",
    "        self.tot_samp = len(self.samples)\n",
    "        np.random.shuffle(self.samples)\n",
    "\n",
    "    def gen_batch(self):\n",
    "        '''Output a batch of training samples'''\n",
    "        n_begin = self.ind\n",
    "        n_end = self.ind + self.batch_size\n",
    "        if n_end >= self.tot_samp:\n",
    "            self.ind = 0\n",
    "            n_begin = self.ind\n",
    "            n_end = self.ind + self.batch_size\n",
    "            self.epoch += 1\n",
    "            if self.epoch % 100 == 0:\n",
    "                self.reinit()\n",
    "        self.ind += self.batch_size\n",
    "        return self.samples[n_begin:n_end]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-17T12:37:44.768347Z",
     "start_time": "2020-04-17T12:37:44.759368Z"
    }
   },
   "outputs": [],
   "source": [
    "data_dir = 'data/train'\n",
    "val_dir = 'data/test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-17T12:37:47.016850Z",
     "start_time": "2020-04-17T12:37:47.005878Z"
    }
   },
   "outputs": [],
   "source": [
    "class DataGenerator2(object):\n",
    "    def __init__(self, pkl_list, batch_size):\n",
    "        '''pkl_list: .pkl files contaiing the data set'''\n",
    "        self.ind = 0  # index of current reading position\n",
    "        self.batch_size = batch_size\n",
    "        self.samples = []\n",
    "        self.epoch = 0\n",
    "        # read in all the .pkl files\n",
    "        for pkl in pkl_list:\n",
    "            self.samples.extend(pickle.load(open(pkl, 'rb')))\n",
    "        self.tot_samp = len(self.samples)\n",
    "        print(self.tot_samp)\n",
    "        print('samples')\n",
    "        np.random.shuffle(self.samples)\n",
    "\n",
    "    def gen_batch(self):\n",
    "        # generate a batch of data\n",
    "        n_begin = self.ind\n",
    "        n_end = self.ind + self.batch_size\n",
    "        if n_end >= self.tot_samp:\n",
    "            # rewire the index\n",
    "            self.ind = 0\n",
    "            n_begin = self.ind\n",
    "            n_end = self.ind + self.batch_size\n",
    "            self.epoch += 1\n",
    "            np.random.shuffle(self.samples)\n",
    "        self.ind += self.batch_size\n",
    "        return self.samples[n_begin:n_end]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-17T12:39:48.804119Z",
     "start_time": "2020-04-17T12:39:48.765224Z"
    }
   },
   "outputs": [],
   "source": [
    "class Model(object):\n",
    "    def __init__(self, n_hidden, batch_size, p_keep_ff, p_keep_rc):\n",
    "        '''n_hidden: number of hidden states\n",
    "           p_keep_ff: forward keep probability\n",
    "           p_keep_rc: recurrent keep probability'''\n",
    "        self.n_hidden = n_hidden\n",
    "        self.batch_size = batch_size\n",
    "        self.p_keep_ff = p_keep_ff\n",
    "        self.p_keep_rc = p_keep_rc\n",
    "        # biases and weights for the last layer\n",
    "        self.weights = {'out': tf.Variable(tf.random_normal([2 * n_hidden, EMBBEDDING_D * NEFF]))}\n",
    "        self.biases = {'out': tf.Variable(tf.random_normal([EMBBEDDING_D * NEFF]))}\n",
    "\n",
    "    def inference(self, x):\n",
    "        '''The structure of the network'''\n",
    "        # four layer of LSTM cell blocks\n",
    "        with tf.variable_scope('BLSTM1') as scope:\n",
    "            lstm_fw_cell = tf.contrib.rnn.LayerNormBasicLSTMCell(\n",
    "                self.n_hidden, layer_norm=False,\n",
    "                dropout_keep_prob=self.p_keep_rc) \n",
    "            lstm_fw_cell = tf.nn.rnn_cell.DropoutWrapper(\n",
    "                lstm_fw_cell, input_keep_prob=1,\n",
    "                output_keep_prob=self.p_keep_ff)\n",
    "            lstm_bw_cell = tf.contrib.rnn.LayerNormBasicLSTMCell(\n",
    "                self.n_hidden, layer_norm=False,\n",
    "                dropout_keep_prob=self.p_keep_rc)\n",
    "            lstm_bw_cell = tf.nn.rnn_cell.DropoutWrapper(\n",
    "                lstm_bw_cell, input_keep_prob=1,\n",
    "                output_keep_prob=self.p_keep_ff)\n",
    "            outputs, _ = tf.nn.bidirectional_dynamic_rnn(\n",
    "                lstm_fw_cell, lstm_bw_cell, x,\n",
    "                sequence_length=[FRAMES_PER_SAMPLE] * self.batch_size,\n",
    "                dtype=tf.float32)\n",
    "            state_concate = tf.concat(outputs, 2)\n",
    "        with tf.variable_scope('BLSTM2') as scope:\n",
    "            lstm_fw_cell2 = tf.contrib.rnn.LayerNormBasicLSTMCell(\n",
    "                self.n_hidden, layer_norm=False,\n",
    "                dropout_keep_prob=self.p_keep_rc)\n",
    "            lstm_fw_cell2 = tf.nn.rnn_cell.DropoutWrapper(\n",
    "                lstm_fw_cell2, input_keep_prob=1,\n",
    "                output_keep_prob=self.p_keep_ff)\n",
    "            lstm_bw_cell2 = tf.contrib.rnn.LayerNormBasicLSTMCell(\n",
    "                self.n_hidden, layer_norm=False,\n",
    "                dropout_keep_prob=self.p_keep_rc)\n",
    "            lstm_bw_cell2 = tf.nn.rnn_cell.DropoutWrapper(\n",
    "                lstm_bw_cell2, input_keep_prob=1,\n",
    "                output_keep_prob=self.p_keep_ff)\n",
    "            outputs2, _ = tf.nn.bidirectional_dynamic_rnn(\n",
    "                lstm_fw_cell2, lstm_bw_cell2, state_concate,\n",
    "                sequence_length=[FRAMES_PER_SAMPLE] * self.batch_size,\n",
    "                dtype=tf.float32)\n",
    "            state_concate2 = tf.concat(outputs2, 2)\n",
    "        with tf.variable_scope('BLSTM3') as scope:\n",
    "            lstm_fw_cell3 = tf.contrib.rnn.LayerNormBasicLSTMCell(\n",
    "                self.n_hidden, layer_norm=False,\n",
    "                dropout_keep_prob=self.p_keep_rc)\n",
    "            lstm_fw_cell3 = tf.nn.rnn_cell.DropoutWrapper(\n",
    "                lstm_fw_cell3, input_keep_prob=1,\n",
    "                output_keep_prob=self.p_keep_ff)\n",
    "            lstm_bw_cell3 = tf.contrib.rnn.LayerNormBasicLSTMCell(\n",
    "                self.n_hidden, layer_norm=False,\n",
    "                dropout_keep_prob=self.p_keep_rc)\n",
    "            lstm_bw_cell3 = tf.nn.rnn_cell.DropoutWrapper(\n",
    "                lstm_bw_cell3, input_keep_prob=1,\n",
    "                output_keep_prob=self.p_keep_ff)\n",
    "            outputs3, _ = tf.nn.bidirectional_dynamic_rnn(\n",
    "                lstm_fw_cell3, lstm_bw_cell3, state_concate2,\n",
    "                sequence_length=[FRAMES_PER_SAMPLE] * self.batch_size,\n",
    "                dtype=tf.float32)\n",
    "            state_concate3 = tf.concat(outputs3, 2)\n",
    "        with tf.variable_scope('BLSTM4') as scope:\n",
    "            lstm_fw_cell4 = tf.contrib.rnn.LayerNormBasicLSTMCell(\n",
    "                self.n_hidden, layer_norm=False,\n",
    "                dropout_keep_prob=self.p_keep_rc)\n",
    "            lstm_fw_cell4 = tf.nn.rnn_cell.DropoutWrapper(\n",
    "                lstm_fw_cell4, input_keep_prob=1,\n",
    "                output_keep_prob=self.p_keep_ff)\n",
    "            lstm_bw_cell4 = tf.contrib.rnn.LayerNormBasicLSTMCell(\n",
    "                self.n_hidden, layer_norm=False,\n",
    "                dropout_keep_prob=self.p_keep_rc)\n",
    "            lstm_bw_cell4 = tf.nn.rnn_cell.DropoutWrapper(\n",
    "                lstm_bw_cell4, input_keep_prob=1,\n",
    "                output_keep_prob=self.p_keep_ff)\n",
    "            outputs4, _ = tf.nn.bidirectional_dynamic_rnn(\n",
    "                lstm_fw_cell4, lstm_bw_cell4, state_concate3,\n",
    "                sequence_length=[FRAMES_PER_SAMPLE] * self.batch_size,\n",
    "                dtype=tf.float32)\n",
    "            state_concate4 = tf.concat(outputs4, 2)\n",
    "        # one layer of embedding output with tanh activation function\n",
    "        out_concate = tf.reshape(state_concate4, [-1, self.n_hidden * 2])\n",
    "        emb_out = tf.matmul(out_concate,self.weights['out']) + self.biases['out']\n",
    "        emb_out = tf.nn.tanh(emb_out)\n",
    "        reshaped_emb = tf.reshape(emb_out, [-1, NEFF, EMBBEDDING_D])\n",
    "        # normalization before output\n",
    "        normalized_emb = tf.nn.l2_normalize(reshaped_emb, 2)\n",
    "        return normalized_emb\n",
    "\n",
    "    def loss(self, embeddings, Y, VAD):\n",
    "        '''Defining the loss function'''\n",
    "        embeddings_rs = tf.reshape(embeddings, shape=[-1, EMBBEDDING_D])\n",
    "        VAD_rs = tf.reshape(VAD, shape=[-1])\n",
    "        # get the embeddings with active VAD\n",
    "        embeddings_rsv = tf.transpose(tf.multiply(tf.transpose(embeddings_rs), VAD_rs))\n",
    "        embeddings_v = tf.reshape(embeddings_rsv, [-1, FRAMES_PER_SAMPLE * NEFF, EMBBEDDING_D])\n",
    "        # get the Y(speaker indicator function) with active VAD\n",
    "        Y_rs = tf.reshape(Y, shape=[-1, 2])\n",
    "        Y_rsv = tf.transpose(tf.multiply(tf.transpose(Y_rs), VAD_rs))\n",
    "        Y_v = tf.reshape(Y_rsv, shape=[-1, FRAMES_PER_SAMPLE * NEFF, 2])\n",
    "        # fast computation format of the embedding loss function\n",
    "        loss_batch = tf.nn.l2_loss(tf.matmul(tf.transpose(embeddings_v, [0, 2, 1]), embeddings_v)) - \\\n",
    "            2 * tf.nn.l2_loss(tf.matmul(tf.transpose(embeddings_v, [0, 2, 1]), Y_v)) + \\\n",
    "            tf.nn.l2_loss(tf.matmul(tf.transpose(Y_v, [0, 2, 1]), Y_v))\n",
    "\n",
    "        loss_v = (loss_batch) / self.batch_size\n",
    "        tf.summary.scalar('loss', loss_v)\n",
    "        return loss_v\n",
    "\n",
    "    def train(self, loss, lr):\n",
    "        '''Optimizer'''\n",
    "        optimizer = tf.train.AdamOptimizer(learning_rate=lr,beta1=0.9,beta2=0.999,epsilon=1e-8)\n",
    "        gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "        gradients, _ = tf.clip_by_global_norm(gradients, 200)\n",
    "        train_op = optimizer.apply_gradients(zip(gradients, v))\n",
    "        return train_op"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-17T12:41:33.163540Z",
     "start_time": "2020-04-17T12:41:33.157522Z"
    }
   },
   "outputs": [],
   "source": [
    "pkl_list = ['data/train/train_01.pkl']\n",
    "val_list = ['data/test/val.pkl']\n",
    "sum_dir = 'sum'\n",
    "train_dir = 'train'\n",
    "\n",
    "lr = 1e-6\n",
    "n_hidden = 300\n",
    "max_steps = 900100\n",
    "batch_size = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-17T12:40:54.665688Z",
     "start_time": "2020-04-17T12:40:54.634771Z"
    }
   },
   "outputs": [],
   "source": [
    "def train():\n",
    "    with tf.Graph().as_default():\n",
    "        # dropout keep probability\n",
    "        p_keep_ff = tf.placeholder(tf.float32, shape=None)\n",
    "        p_keep_rc = tf.placeholder(tf.float32, shape=None)\n",
    "        # generator for training set and validation set\n",
    "        data_generator = DataGenerator2(pkl_list, batch_size)\n",
    "        val_generator = DataGenerator2(val_list, batch_size)\n",
    "        print('datagenerator finish')\n",
    "        # placeholder for input log spectrum, VAD info.,\n",
    "        # and speaker indicator function\n",
    "        in_data = tf.placeholder(tf.float32, shape=[batch_size, FRAMES_PER_SAMPLE, NEFF])\n",
    "        VAD_data = tf.placeholder(tf.float32, shape=[batch_size, FRAMES_PER_SAMPLE, NEFF])\n",
    "        Y_data = tf.placeholder(tf.float32, shape=[batch_size, FRAMES_PER_SAMPLE, NEFF, 2])\n",
    "        # init the model\n",
    "        BiModel = Model(n_hidden, batch_size, p_keep_ff, p_keep_rc)\n",
    "        # build the net structure\n",
    "        embedding = BiModel.inference(in_data)\n",
    "        print('model build finish')\n",
    "        Y_data_reshaped = tf.reshape(Y_data, [-1, NEFF, 2])\n",
    "        VAD_data_reshaped = tf.reshape(VAD_data, [-1, NEFF])\n",
    "        # compute the loss\n",
    "        loss = BiModel.loss(embedding, Y_data_reshaped, VAD_data_reshaped)\n",
    "        print('loss build finish')\n",
    "        # get the train operation\n",
    "        train_op = BiModel.train(loss, lr)\n",
    "        saver = tf.train.Saver(tf.global_variables())\n",
    "        summary_op = tf.summary.merge_all()\n",
    "        sess = tf.Session()\n",
    "\n",
    "        # either train from scratch or a trained model\n",
    "        saver.restore(sess, 'train/484000model.ckpt-484000')\n",
    "        val_loss = np.fromfile('val_loss').tolist()\n",
    "        init_step = 484001\n",
    "        #init = tf.global_variables_initializer()\n",
    "        #sess.run(init)\n",
    "        #init_step = 0\n",
    "\n",
    "        summary_writer = tf.summary.FileWriter(sum_dir, sess.graph)\n",
    "\n",
    "        last_epoch = data_generator.epoch\n",
    "\n",
    "        for step in range(init_step, max_steps):\n",
    "            start_time = time.time()\n",
    "            data_batch = data_generator.gen_batch()\n",
    "            # concatenate the samples into batch data\n",
    "            in_data_np = np.concatenate([np.reshape(item['Sample'], [1, FRAMES_PER_SAMPLE, NEFF]) for item in data_batch])\n",
    "            VAD_data_np = np.concatenate([np.reshape(item['VAD'], [1, FRAMES_PER_SAMPLE, NEFF]) for item in data_batch])\n",
    "            VAD_data_np = VAD_data_np.astype('int')\n",
    "            Y_data_np = np.concatenate([np.reshape(item['Target'], [1, FRAMES_PER_SAMPLE, NEFF, 2]) for item in data_batch])\n",
    "            Y_data_np = Y_data_np.astype('int')\n",
    "            # train the model\n",
    "            loss_value, _, summary_str = sess.run([loss, train_op, summary_op],feed_dict={in_data: in_data_np, \\\n",
    "                                                                                          VAD_data: VAD_data_np, \\\n",
    "                                                                                          Y_data: Y_data_np, \\\n",
    "                                                                                          p_keep_ff: 1 - P_DROPOUT_FF, \\\n",
    "                                                                                          p_keep_rc: 1 - P_DROPOUT_RC})\n",
    "            summary_writer.add_summary(summary_str, step)\n",
    "            duration = time.time() - start_time\n",
    "            assert not np.isnan(loss_value)\n",
    "            if step % 100 == 0:\n",
    "                # show training progress every 100 steps\n",
    "                num_examples_per_step = batch_size\n",
    "                examples_per_sec = num_examples_per_step / duration\n",
    "                sec_per_batch = float(duration)\n",
    "\n",
    "                format_str = (\n",
    "                    '%s: step %d, loss = %.2f (%.1f examples/sec; %.3f '\n",
    "                    'sec/batch, epoch %d)')\n",
    "                print (format_str % (datetime.now(), step, loss_value,\n",
    "                                     examples_per_sec, sec_per_batch,\n",
    "                                     data_generator.epoch))\n",
    "            if step % 4000 == 0 and step != 0:\n",
    "                # save model every 4000 steps\n",
    "                checkpoint_path = os.path.join(train_dir, str(step)+'model.ckpt')\n",
    "                saver.save(sess, checkpoint_path, global_step=step)\n",
    "                print('step: ', step, 'save model: ', train_dir+str(step)+'model.ckpt')\n",
    "\n",
    "            if last_epoch != data_generator.epoch:\n",
    "                # doing validation every training epoch\n",
    "                print('Doing validation')\n",
    "                val_epoch = val_generator.epoch\n",
    "                count = 0\n",
    "                loss_sum = 0\n",
    "                val_loss = []\n",
    "                # average the validation loss\n",
    "                while(val_epoch == val_generator.epoch):\n",
    "                    count += 1\n",
    "                    data_batch = val_generator.gen_batch()\n",
    "                    in_data_np = np.concatenate([np.reshape(item['Sample'],[1, FRAMES_PER_SAMPLE, NEFF]) for item in data_batch])\n",
    "                    VAD_data_np = np.concatenate([np.reshape(item['VAD'],[1, FRAMES_PER_SAMPLE, NEFF]) for item in data_batch])\n",
    "                    VAD_data_np = VAD_data_np.astype('int')\n",
    "                    Y_data_np = np.concatenate([np.reshape(item['Target'],[1, FRAMES_PER_SAMPLE, NEFF, 2]) for item in data_batch])\n",
    "                    Y_data_np = Y_data_np.astype('int')\n",
    "                    loss_value, = sess.run(\n",
    "                        [loss],\n",
    "                        feed_dict={in_data: in_data_np,\n",
    "                                   VAD_data: VAD_data_np,\n",
    "                                   Y_data: Y_data_np,\n",
    "                                   p_keep_ff: 1,\n",
    "                                   p_keep_rc: 1})\n",
    "                    loss_sum += loss_value\n",
    "                val_loss.append(loss_sum / count)\n",
    "                print ('validation loss: %.3f' % (loss_sum / count))\n",
    "                np.array(val_loss).tofile('val_loss')\n",
    "\n",
    "            last_epoch = data_generator.epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-02-18T04:58:23.890Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-02-18 10:28:23.892747 start\n",
      "2437\n",
      "samples\n",
      "477\n",
      "samples\n",
      "datagenerator finish\n",
      "WARNING:tensorflow:\n",
      "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "  * https://github.com/tensorflow/io (for I/O related ops)\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n",
      "WARNING:tensorflow:From <ipython-input-10-a8f97669e1f9>:50: bidirectional_dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `keras.layers.Bidirectional(keras.layers.RNN(cell))`, which is equivalent to this API\n",
      "WARNING:tensorflow:From C:\\Users\\91828\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\rnn.py:464: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `keras.layers.RNN(cell)`, which is equivalent to this API\n",
      "WARNING:tensorflow:From C:\\Users\\91828\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From C:\\Users\\91828\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\autograph\\impl\\api.py:253: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:From C:\\Users\\91828\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\rnn.py:244: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "model build finish\n",
      "loss build finish\n",
      "WARNING:tensorflow:From C:\\Users\\91828\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\saver.py:1276: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use standard file APIs to check for files with this prefix.\n",
      "INFO:tensorflow:Restoring parameters from train/440000model.ckpt-440000\n",
      "2020-02-18 10:30:13.363851: step 440100, loss = 1006296.00 (19.0 examples/sec; 0.842 sec/batch, epoch 0)\n",
      "Doing validation\n",
      "validation loss: 5257842.783\n",
      "2020-02-18 10:31:51.461503: step 440200, loss = 1680955.00 (14.8 examples/sec; 1.081 sec/batch, epoch 1)\n",
      "2020-02-18 10:33:19.841217: step 440300, loss = 1415547.00 (19.4 examples/sec; 0.827 sec/batch, epoch 1)\n",
      "Doing validation\n",
      "validation loss: 5221530.379\n",
      "2020-02-18 10:34:56.961536: step 440400, loss = 1114819.50 (18.8 examples/sec; 0.851 sec/batch, epoch 2)\n",
      "Doing validation\n",
      "validation loss: 5283601.241\n",
      "2020-02-18 10:36:34.476787: step 440500, loss = 1295525.00 (18.7 examples/sec; 0.856 sec/batch, epoch 3)\n",
      "2020-02-18 10:37:59.265346: step 440600, loss = 1283990.50 (19.4 examples/sec; 0.823 sec/batch, epoch 3)\n",
      "Doing validation\n",
      "validation loss: 5312986.310\n",
      "2020-02-18 10:39:35.041675: step 440700, loss = 1430785.00 (19.1 examples/sec; 0.839 sec/batch, epoch 4)\n",
      "Doing validation\n",
      "validation loss: 5198962.724\n",
      "2020-02-18 10:41:10.830541: step 440800, loss = 1262211.00 (18.7 examples/sec; 0.856 sec/batch, epoch 5)\n",
      "2020-02-18 10:42:34.910761: step 440900, loss = 1373044.00 (19.0 examples/sec; 0.842 sec/batch, epoch 5)\n",
      "Doing validation\n",
      "validation loss: 5306526.552\n",
      "2020-02-18 10:44:13.226824: step 441000, loss = 1279372.00 (19.1 examples/sec; 0.840 sec/batch, epoch 6)\n",
      "Doing validation\n",
      "validation loss: 5234988.707\n",
      "2020-02-18 10:45:39.165060: step 441100, loss = 1394947.00 (22.5 examples/sec; 0.711 sec/batch, epoch 7)\n",
      "2020-02-18 10:46:45.552774: step 441200, loss = 1360096.00 (24.1 examples/sec; 0.665 sec/batch, epoch 7)\n",
      "Doing validation\n",
      "validation loss: 5229205.103\n",
      "2020-02-18 10:48:00.995082: step 441300, loss = 1529181.00 (24.2 examples/sec; 0.661 sec/batch, epoch 8)\n",
      "Doing validation\n",
      "validation loss: 5250535.293\n",
      "2020-02-18 10:49:16.201736: step 441400, loss = 1275074.00 (24.1 examples/sec; 0.663 sec/batch, epoch 9)\n",
      "2020-02-18 10:50:22.526063: step 441500, loss = 1162412.50 (24.3 examples/sec; 0.659 sec/batch, epoch 9)\n",
      "Doing validation\n",
      "validation loss: 5256373.776\n",
      "2020-02-18 10:51:37.594443: step 441600, loss = 1123005.00 (24.2 examples/sec; 0.661 sec/batch, epoch 10)\n",
      "Doing validation\n",
      "validation loss: 5266505.103\n",
      "2020-02-18 10:52:53.028333: step 441700, loss = 1133951.50 (23.8 examples/sec; 0.673 sec/batch, epoch 11)\n",
      "2020-02-18 10:53:59.317298: step 441800, loss = 1033039.50 (23.8 examples/sec; 0.671 sec/batch, epoch 11)\n",
      "Doing validation\n",
      "validation loss: 5225401.466\n",
      "2020-02-18 10:55:14.641960: step 441900, loss = 1371733.00 (23.9 examples/sec; 0.670 sec/batch, epoch 12)\n",
      "Doing validation\n",
      "validation loss: 5223287.000\n",
      "2020-02-18 10:56:29.816425: step 442000, loss = 1606149.00 (23.9 examples/sec; 0.669 sec/batch, epoch 13)\n",
      "2020-02-18 10:57:36.213852: step 442100, loss = 1258659.50 (23.9 examples/sec; 0.669 sec/batch, epoch 13)\n",
      "Doing validation\n",
      "validation loss: 5282246.603\n",
      "2020-02-18 10:58:51.799771: step 442200, loss = 988646.00 (23.9 examples/sec; 0.670 sec/batch, epoch 14)\n",
      "Doing validation\n",
      "validation loss: 5233500.948\n",
      "2020-02-18 11:00:08.690482: step 442300, loss = 1164530.50 (24.0 examples/sec; 0.666 sec/batch, epoch 15)\n",
      "2020-02-18 11:01:15.963055: step 442400, loss = 1546097.50 (24.3 examples/sec; 0.657 sec/batch, epoch 15)\n",
      "Doing validation\n",
      "validation loss: 5217026.810\n",
      "2020-02-18 11:02:32.114463: step 442500, loss = 1338850.00 (23.4 examples/sec; 0.684 sec/batch, epoch 16)\n",
      "Doing validation\n",
      "validation loss: 5261336.931\n",
      "2020-02-18 11:03:51.578702: step 442600, loss = 1294524.50 (23.6 examples/sec; 0.679 sec/batch, epoch 17)\n",
      "2020-02-18 11:04:58.405612: step 442700, loss = 1175040.00 (23.9 examples/sec; 0.668 sec/batch, epoch 17)\n",
      "Doing validation\n",
      "validation loss: 5284711.017\n",
      "2020-02-18 11:06:14.939602: step 442800, loss = 1680060.00 (23.8 examples/sec; 0.671 sec/batch, epoch 18)\n",
      "Doing validation\n",
      "validation loss: 5243747.345\n",
      "2020-02-18 11:07:30.733362: step 442900, loss = 1474850.50 (24.1 examples/sec; 0.665 sec/batch, epoch 19)\n",
      "2020-02-18 11:08:37.946322: step 443000, loss = 1427884.00 (24.1 examples/sec; 0.664 sec/batch, epoch 19)\n",
      "Doing validation\n",
      "validation loss: 5284492.276\n",
      "2020-02-18 11:09:53.544469: step 443100, loss = 1526029.00 (24.1 examples/sec; 0.664 sec/batch, epoch 20)\n",
      "Doing validation\n",
      "validation loss: 5242589.379\n",
      "2020-02-18 11:11:09.786719: step 443200, loss = 1632347.00 (23.9 examples/sec; 0.668 sec/batch, epoch 21)\n",
      "2020-02-18 11:12:16.512278: step 443300, loss = 1322929.00 (23.7 examples/sec; 0.674 sec/batch, epoch 21)\n",
      "Doing validation\n",
      "validation loss: 5188220.586\n",
      "2020-02-18 11:13:32.116722: step 443400, loss = 1266759.50 (24.3 examples/sec; 0.657 sec/batch, epoch 22)\n",
      "Doing validation\n",
      "validation loss: 5247733.914\n",
      "2020-02-18 11:14:47.885129: step 443500, loss = 1152317.00 (24.3 examples/sec; 0.658 sec/batch, epoch 23)\n",
      "2020-02-18 11:15:54.520649: step 443600, loss = 1079281.00 (24.1 examples/sec; 0.665 sec/batch, epoch 23)\n",
      "Doing validation\n",
      "validation loss: 5322887.500\n",
      "2020-02-18 11:17:10.152468: step 443700, loss = 1372090.50 (24.0 examples/sec; 0.667 sec/batch, epoch 24)\n",
      "2020-02-18 11:18:16.868023: step 443800, loss = 1184859.00 (24.0 examples/sec; 0.666 sec/batch, epoch 24)\n",
      "Doing validation\n",
      "validation loss: 5224088.121\n",
      "2020-02-18 11:19:32.478842: step 443900, loss = 1137779.50 (24.2 examples/sec; 0.660 sec/batch, epoch 25)\n",
      "Doing validation\n",
      "validation loss: 5267029.517\n",
      "2020-02-18 11:20:51.496866: step 444000, loss = 1237355.00 (23.9 examples/sec; 0.668 sec/batch, epoch 26)\n",
      "step:  444000 save model:  train444000model.ckpt\n",
      "2020-02-18 11:22:02.062635: step 444100, loss = 1105658.00 (23.4 examples/sec; 0.685 sec/batch, epoch 26)\n",
      "Doing validation\n",
      "validation loss: 5250799.828\n",
      "2020-02-18 11:23:18.171129: step 444200, loss = 1222379.00 (21.4 examples/sec; 0.748 sec/batch, epoch 27)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doing validation\n",
      "validation loss: 5191909.207\n",
      "2020-02-18 11:24:35.247405: step 444300, loss = 1241617.00 (23.4 examples/sec; 0.684 sec/batch, epoch 28)\n",
      "2020-02-18 11:25:43.085011: step 444400, loss = 1527234.50 (23.9 examples/sec; 0.669 sec/batch, epoch 28)\n",
      "Doing validation\n",
      "validation loss: 5325738.138\n",
      "2020-02-18 11:26:59.733623: step 444500, loss = 1478615.50 (23.2 examples/sec; 0.688 sec/batch, epoch 29)\n",
      "Doing validation\n",
      "validation loss: 5264495.121\n",
      "2020-02-18 11:28:17.256649: step 444600, loss = 1686671.00 (23.9 examples/sec; 0.670 sec/batch, epoch 30)\n",
      "2020-02-18 11:29:25.636848: step 444700, loss = 1131297.50 (23.9 examples/sec; 0.668 sec/batch, epoch 30)\n",
      "Doing validation\n",
      "validation loss: 5184003.155\n",
      "2020-02-18 11:30:44.100481: step 444800, loss = 1400299.50 (23.5 examples/sec; 0.680 sec/batch, epoch 31)\n",
      "Doing validation\n",
      "validation loss: 5293459.776\n",
      "2020-02-18 11:32:01.407537: step 444900, loss = 1111875.50 (23.8 examples/sec; 0.671 sec/batch, epoch 32)\n",
      "2020-02-18 11:33:09.869207: step 445000, loss = 1253339.50 (23.8 examples/sec; 0.671 sec/batch, epoch 32)\n",
      "Doing validation\n",
      "validation loss: 5276302.259\n",
      "2020-02-18 11:34:27.738363: step 445100, loss = 1560449.50 (23.9 examples/sec; 0.670 sec/batch, epoch 33)\n",
      "Doing validation\n",
      "validation loss: 5235307.914\n",
      "2020-02-18 11:35:44.006439: step 445200, loss = 1332314.00 (23.6 examples/sec; 0.677 sec/batch, epoch 34)\n",
      "2020-02-18 11:36:51.164815: step 445300, loss = 1013310.50 (23.4 examples/sec; 0.684 sec/batch, epoch 34)\n",
      "Doing validation\n",
      "validation loss: 5217148.724\n",
      "2020-02-18 11:38:07.437293: step 445400, loss = 1214381.50 (23.8 examples/sec; 0.672 sec/batch, epoch 35)\n",
      "Doing validation\n",
      "validation loss: 5294468.466\n",
      "2020-02-18 11:39:23.568982: step 445500, loss = 1188834.50 (23.5 examples/sec; 0.681 sec/batch, epoch 36)\n",
      "2020-02-18 11:40:30.759338: step 445600, loss = 1511390.00 (23.7 examples/sec; 0.674 sec/batch, epoch 36)\n",
      "Doing validation\n",
      "validation loss: 5195667.810\n",
      "2020-02-18 11:41:46.937748: step 445700, loss = 1325648.50 (23.7 examples/sec; 0.674 sec/batch, epoch 37)\n",
      "Doing validation\n",
      "validation loss: 5224780.397\n",
      "2020-02-18 11:43:04.092506: step 445800, loss = 1269897.00 (24.1 examples/sec; 0.664 sec/batch, epoch 38)\n",
      "2020-02-18 11:44:11.661794: step 445900, loss = 1193104.00 (24.1 examples/sec; 0.665 sec/batch, epoch 38)\n",
      "Doing validation\n",
      "validation loss: 5269792.103\n",
      "2020-02-18 11:45:30.682927: step 446000, loss = 1364092.00 (23.4 examples/sec; 0.683 sec/batch, epoch 39)\n",
      "Doing validation\n",
      "validation loss: 5165866.845\n",
      "2020-02-18 11:46:48.133319: step 446100, loss = 1219233.50 (23.6 examples/sec; 0.679 sec/batch, epoch 40)\n",
      "2020-02-18 11:47:56.924306: step 446200, loss = 1323523.00 (23.3 examples/sec; 0.687 sec/batch, epoch 40)\n",
      "Doing validation\n",
      "validation loss: 5263789.138\n",
      "2020-02-18 11:49:15.118335: step 446300, loss = 1367126.50 (23.9 examples/sec; 0.669 sec/batch, epoch 41)\n",
      "Doing validation\n",
      "validation loss: 5252378.414\n",
      "2020-02-18 11:50:31.764961: step 446400, loss = 1245660.50 (24.1 examples/sec; 0.664 sec/batch, epoch 42)\n",
      "2020-02-18 11:51:38.920613: step 446500, loss = 1423711.00 (24.2 examples/sec; 0.662 sec/batch, epoch 42)\n",
      "Doing validation\n",
      "validation loss: 5268304.845\n",
      "2020-02-18 11:52:55.731558: step 446600, loss = 1132656.50 (24.0 examples/sec; 0.666 sec/batch, epoch 43)\n",
      "Doing validation\n",
      "validation loss: 5230676.431\n",
      "2020-02-18 11:54:15.503023: step 446700, loss = 1305601.50 (24.0 examples/sec; 0.666 sec/batch, epoch 44)\n",
      "2020-02-18 11:55:23.272495: step 446800, loss = 1510224.00 (24.1 examples/sec; 0.663 sec/batch, epoch 44)\n",
      "Doing validation\n",
      "validation loss: 5263785.345\n",
      "2020-02-18 11:56:45.520570: step 446900, loss = 1257610.00 (20.5 examples/sec; 0.780 sec/batch, epoch 45)\n",
      "Doing validation\n",
      "validation loss: 5264235.776\n",
      "2020-02-18 11:58:05.785943: step 447000, loss = 1225196.50 (23.4 examples/sec; 0.684 sec/batch, epoch 46)\n",
      "2020-02-18 11:59:14.093224: step 447100, loss = 1179462.00 (23.8 examples/sec; 0.673 sec/batch, epoch 46)\n",
      "Doing validation\n",
      "validation loss: 5255790.983\n",
      "2020-02-18 12:00:33.799965: step 447200, loss = 1155288.00 (23.5 examples/sec; 0.682 sec/batch, epoch 47)\n",
      "Doing validation\n",
      "validation loss: 5258446.983\n",
      "2020-02-18 12:01:51.124473: step 447300, loss = 1182569.50 (23.5 examples/sec; 0.681 sec/batch, epoch 48)\n",
      "2020-02-18 12:02:58.775202: step 447400, loss = 1289853.00 (24.0 examples/sec; 0.667 sec/batch, epoch 48)\n",
      "Doing validation\n",
      "validation loss: 5262149.397\n",
      "2020-02-18 12:04:14.719738: step 447500, loss = 1355652.00 (24.0 examples/sec; 0.666 sec/batch, epoch 49)\n",
      "2020-02-18 12:05:22.016409: step 447600, loss = 1450136.00 (23.8 examples/sec; 0.673 sec/batch, epoch 49)\n",
      "Doing validation\n",
      "validation loss: 5280011.517\n",
      "2020-02-18 12:06:38.132535: step 447700, loss = 1390437.50 (24.2 examples/sec; 0.662 sec/batch, epoch 50)\n",
      "Doing validation\n",
      "validation loss: 5230265.621\n",
      "2020-02-18 12:07:57.898011: step 447800, loss = 1358457.50 (23.7 examples/sec; 0.674 sec/batch, epoch 51)\n",
      "2020-02-18 12:09:05.247922: step 447900, loss = 1560321.00 (23.9 examples/sec; 0.668 sec/batch, epoch 51)\n",
      "Doing validation\n",
      "validation loss: 5289493.397\n",
      "2020-02-18 12:10:22.949592: step 448000, loss = 1046093.00 (22.9 examples/sec; 0.698 sec/batch, epoch 52)\n",
      "step:  448000 save model:  train448000model.ckpt\n",
      "Doing validation\n",
      "validation loss: 5253603.224\n",
      "2020-02-18 12:11:45.874835: step 448100, loss = 1187117.50 (23.9 examples/sec; 0.668 sec/batch, epoch 53)\n",
      "2020-02-18 12:12:55.163575: step 448200, loss = 1361644.00 (23.8 examples/sec; 0.673 sec/batch, epoch 53)\n",
      "Doing validation\n",
      "validation loss: 5271557.966\n",
      "2020-02-18 12:14:13.745681: step 448300, loss = 1133852.50 (22.7 examples/sec; 0.704 sec/batch, epoch 54)\n",
      "Doing validation\n",
      "validation loss: 5227229.483\n",
      "2020-02-18 12:15:32.246894: step 448400, loss = 1041935.50 (23.9 examples/sec; 0.668 sec/batch, epoch 55)\n",
      "2020-02-18 12:16:40.746890: step 448500, loss = 1194178.50 (24.1 examples/sec; 0.665 sec/batch, epoch 55)\n",
      "Doing validation\n",
      "validation loss: 5287303.086\n",
      "2020-02-18 12:17:57.792463: step 448600, loss = 1492728.00 (23.9 examples/sec; 0.668 sec/batch, epoch 56)\n",
      "Doing validation\n",
      "validation loss: 5282208.138\n",
      "2020-02-18 12:19:14.292904: step 448700, loss = 1197832.00 (24.0 examples/sec; 0.666 sec/batch, epoch 57)\n",
      "2020-02-18 12:20:24.081490: step 448800, loss = 1161857.00 (23.1 examples/sec; 0.693 sec/batch, epoch 57)\n",
      "Doing validation\n",
      "validation loss: 5215180.362\n",
      "2020-02-18 12:21:43.302878: step 448900, loss = 1114937.50 (22.0 examples/sec; 0.728 sec/batch, epoch 58)\n",
      "Doing validation\n",
      "validation loss: 5222828.241\n",
      "2020-02-18 12:23:03.681950: step 449000, loss = 1030032.00 (24.1 examples/sec; 0.663 sec/batch, epoch 59)\n",
      "2020-02-18 12:24:15.878173: step 449100, loss = 977592.00 (23.7 examples/sec; 0.676 sec/batch, epoch 59)\n",
      "Doing validation\n",
      "validation loss: 5251752.466\n",
      "2020-02-18 12:25:36.288687: step 449200, loss = 1378825.00 (23.5 examples/sec; 0.682 sec/batch, epoch 60)\n",
      "Doing validation\n",
      "validation loss: 5244555.121\n",
      "2020-02-18 12:26:54.719973: step 449300, loss = 915948.50 (20.0 examples/sec; 0.799 sec/batch, epoch 61)\n",
      "2020-02-18 12:28:05.276310: step 449400, loss = 1335321.50 (22.0 examples/sec; 0.728 sec/batch, epoch 61)\n",
      "Doing validation\n",
      "validation loss: 5247843.707\n",
      "2020-02-18 12:29:24.873582: step 449500, loss = 1527194.50 (23.7 examples/sec; 0.674 sec/batch, epoch 62)\n",
      "Doing validation\n",
      "validation loss: 5218712.948\n",
      "2020-02-18 12:31:10.517684: step 449600, loss = 1515259.00 (17.3 examples/sec; 0.927 sec/batch, epoch 63)\n",
      "2020-02-18 12:32:43.199153: step 449700, loss = 1299191.50 (17.4 examples/sec; 0.918 sec/batch, epoch 63)\n",
      "Doing validation\n",
      "validation loss: 5303835.379\n",
      "2020-02-18 12:34:43.529902: step 449800, loss = 1247926.00 (14.1 examples/sec; 1.131 sec/batch, epoch 64)\n",
      "Doing validation\n",
      "validation loss: 5220964.862\n",
      "2020-02-18 12:36:36.887657: step 449900, loss = 1659237.00 (16.0 examples/sec; 0.997 sec/batch, epoch 65)\n",
      "2020-02-18 12:38:21.319842: step 450000, loss = 1088979.00 (15.7 examples/sec; 1.022 sec/batch, epoch 65)\n",
      "Doing validation\n",
      "validation loss: 5245390.948\n",
      "2020-02-18 12:40:11.064474: step 450100, loss = 1182769.00 (17.4 examples/sec; 0.921 sec/batch, epoch 66)\n",
      "Doing validation\n",
      "validation loss: 5241981.293\n",
      "2020-02-18 12:41:55.851426: step 450200, loss = 1306575.00 (17.9 examples/sec; 0.894 sec/batch, epoch 67)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-02-18 12:43:30.941131: step 450300, loss = 1275813.50 (17.3 examples/sec; 0.927 sec/batch, epoch 67)\n",
      "Doing validation\n",
      "validation loss: 5286602.810\n",
      "2020-02-18 12:45:17.457333: step 450400, loss = 1332668.50 (17.4 examples/sec; 0.918 sec/batch, epoch 68)\n",
      "Doing validation\n",
      "validation loss: 5252378.483\n",
      "2020-02-18 12:47:02.574227: step 450500, loss = 1569339.00 (17.8 examples/sec; 0.901 sec/batch, epoch 69)\n",
      "2020-02-18 12:48:38.677253: step 450600, loss = 1427783.00 (17.6 examples/sec; 0.911 sec/batch, epoch 69)\n",
      "Doing validation\n",
      "validation loss: 5254521.500\n",
      "2020-02-18 12:50:25.356001: step 450700, loss = 1068102.00 (17.2 examples/sec; 0.930 sec/batch, epoch 70)\n",
      "Doing validation\n",
      "validation loss: 5275628.879\n",
      "2020-02-18 12:52:10.045823: step 450800, loss = 1477670.00 (17.5 examples/sec; 0.914 sec/batch, epoch 71)\n",
      "2020-02-18 12:53:45.736962: step 450900, loss = 1531982.00 (17.4 examples/sec; 0.920 sec/batch, epoch 71)\n",
      "Doing validation\n",
      "validation loss: 5246236.914\n",
      "2020-02-18 12:55:33.225537: step 451000, loss = 1659474.00 (17.1 examples/sec; 0.936 sec/batch, epoch 72)\n",
      "Doing validation\n",
      "validation loss: 5232734.931\n",
      "2020-02-18 12:57:17.354102: step 451100, loss = 1456751.00 (17.5 examples/sec; 0.913 sec/batch, epoch 73)\n",
      "2020-02-18 12:58:51.610098: step 451200, loss = 1197622.50 (17.9 examples/sec; 0.894 sec/batch, epoch 73)\n",
      "Doing validation\n",
      "validation loss: 5268793.569\n",
      "2020-02-18 13:00:35.912232: step 451300, loss = 1264735.00 (15.8 examples/sec; 1.012 sec/batch, epoch 74)\n",
      "2020-02-18 13:02:10.140371: step 451400, loss = 1127669.00 (17.7 examples/sec; 0.904 sec/batch, epoch 74)\n",
      "Doing validation\n",
      "validation loss: 5276270.897\n",
      "2020-02-18 13:04:02.010268: step 451500, loss = 1409938.50 (17.4 examples/sec; 0.919 sec/batch, epoch 75)\n",
      "Doing validation\n",
      "validation loss: 5215914.517\n",
      "2020-02-18 13:05:52.546790: step 451600, loss = 1190228.00 (19.8 examples/sec; 0.808 sec/batch, epoch 76)\n",
      "2020-02-18 13:07:16.449425: step 451700, loss = 1392607.00 (18.8 examples/sec; 0.853 sec/batch, epoch 76)\n",
      "Doing validation\n",
      "validation loss: 5176316.414\n",
      "2020-02-18 13:08:56.524836: step 451800, loss = 1216957.00 (18.9 examples/sec; 0.847 sec/batch, epoch 77)\n",
      "Doing validation\n",
      "validation loss: 5332387.259\n",
      "2020-02-18 13:10:44.008009: step 451900, loss = 1041406.50 (18.7 examples/sec; 0.857 sec/batch, epoch 78)\n",
      "2020-02-18 13:12:10.536544: step 452000, loss = 1371524.00 (18.5 examples/sec; 0.865 sec/batch, epoch 78)\n",
      "step:  452000 save model:  train452000model.ckpt\n",
      "Doing validation\n",
      "validation loss: 5277555.138\n",
      "2020-02-18 13:13:52.375607: step 452100, loss = 1369623.00 (19.4 examples/sec; 0.825 sec/batch, epoch 79)\n",
      "Doing validation\n",
      "validation loss: 5206546.259\n",
      "2020-02-18 13:15:28.270190: step 452200, loss = 1364756.00 (18.9 examples/sec; 0.848 sec/batch, epoch 80)\n",
      "2020-02-18 13:16:52.308767: step 452300, loss = 966968.00 (19.1 examples/sec; 0.838 sec/batch, epoch 80)\n",
      "Doing validation\n",
      "validation loss: 5244891.724\n",
      "2020-02-18 13:18:28.841644: step 452400, loss = 1689279.00 (19.0 examples/sec; 0.843 sec/batch, epoch 81)\n",
      "Doing validation\n",
      "validation loss: 5263858.552\n",
      "2020-02-18 13:20:05.678883: step 452500, loss = 1417258.00 (18.3 examples/sec; 0.877 sec/batch, epoch 82)\n",
      "2020-02-18 13:21:29.632396: step 452600, loss = 1250473.00 (18.7 examples/sec; 0.855 sec/batch, epoch 82)\n",
      "Doing validation\n",
      "validation loss: 5290334.810\n",
      "2020-02-18 13:23:05.374387: step 452700, loss = 1471813.00 (19.1 examples/sec; 0.836 sec/batch, epoch 83)\n",
      "Doing validation\n",
      "validation loss: 5196443.776\n",
      "2020-02-18 13:24:23.522144: step 452800, loss = 1405521.50 (24.1 examples/sec; 0.664 sec/batch, epoch 84)\n",
      "2020-02-18 13:25:30.344539: step 452900, loss = 1192592.00 (24.3 examples/sec; 0.659 sec/batch, epoch 84)\n",
      "Doing validation\n",
      "validation loss: 5282991.672\n",
      "2020-02-18 13:26:46.127166: step 453000, loss = 1422769.00 (23.9 examples/sec; 0.669 sec/batch, epoch 85)\n",
      "Doing validation\n",
      "validation loss: 5230958.155\n",
      "2020-02-18 13:28:02.071641: step 453100, loss = 1263611.00 (23.5 examples/sec; 0.680 sec/batch, epoch 86)\n",
      "2020-02-18 13:29:08.954697: step 453200, loss = 1626728.00 (23.9 examples/sec; 0.670 sec/batch, epoch 86)\n",
      "Doing validation\n",
      "validation loss: 5237991.500\n",
      "2020-02-18 13:30:24.887164: step 453300, loss = 1161274.00 (24.2 examples/sec; 0.660 sec/batch, epoch 87)\n",
      "Doing validation\n",
      "validation loss: 5207573.724\n",
      "2020-02-18 13:31:40.655564: step 453400, loss = 1509129.00 (23.8 examples/sec; 0.672 sec/batch, epoch 88)\n",
      "2020-02-18 13:32:47.546675: step 453500, loss = 1292983.00 (24.0 examples/sec; 0.666 sec/batch, epoch 88)\n",
      "Doing validation\n",
      "validation loss: 5282350.966\n",
      "2020-02-18 13:34:03.232341: step 453600, loss = 1080492.00 (23.9 examples/sec; 0.670 sec/batch, epoch 89)\n",
      "Doing validation\n",
      "validation loss: 5192472.069\n",
      "2020-02-18 13:35:19.213857: step 453700, loss = 1206915.00 (23.8 examples/sec; 0.673 sec/batch, epoch 90)\n",
      "2020-02-18 13:36:26.098510: step 453800, loss = 1160337.50 (24.0 examples/sec; 0.667 sec/batch, epoch 90)\n",
      "Doing validation\n",
      "validation loss: 5245568.724\n",
      "2020-02-18 13:37:41.898862: step 453900, loss = 1096930.00 (24.1 examples/sec; 0.664 sec/batch, epoch 91)\n",
      "Doing validation\n",
      "validation loss: 5271549.466\n",
      "2020-02-18 13:38:57.768234: step 454000, loss = 1215212.50 (24.2 examples/sec; 0.660 sec/batch, epoch 92)\n",
      "2020-02-18 13:40:04.621489: step 454100, loss = 1331965.00 (23.7 examples/sec; 0.676 sec/batch, epoch 92)\n",
      "Doing validation\n",
      "validation loss: 5235083.259\n",
      "2020-02-18 13:41:20.443829: step 454200, loss = 1238633.00 (23.8 examples/sec; 0.671 sec/batch, epoch 93)\n",
      "Doing validation\n",
      "validation loss: 5325524.983\n",
      "2020-02-18 13:42:36.246472: step 454300, loss = 1134636.50 (23.9 examples/sec; 0.668 sec/batch, epoch 94)\n",
      "2020-02-18 13:43:43.000162: step 454400, loss = 1119148.00 (24.1 examples/sec; 0.665 sec/batch, epoch 94)\n",
      "Doing validation\n",
      "validation loss: 5261765.121\n",
      "2020-02-18 13:44:58.890270: step 454500, loss = 1226090.50 (24.0 examples/sec; 0.666 sec/batch, epoch 95)\n",
      "Doing validation\n",
      "validation loss: 5207467.207\n",
      "2020-02-18 13:46:14.743443: step 454600, loss = 1243755.00 (24.1 examples/sec; 0.664 sec/batch, epoch 96)\n",
      "2020-02-18 13:47:21.514897: step 454700, loss = 1242711.50 (23.8 examples/sec; 0.671 sec/batch, epoch 96)\n",
      "Doing validation\n",
      "validation loss: 5216724.828\n",
      "2020-02-18 13:48:39.472725: step 454800, loss = 1223893.00 (24.0 examples/sec; 0.666 sec/batch, epoch 97)\n",
      "Doing validation\n",
      "validation loss: 5274361.931\n",
      "2020-02-18 13:49:55.271998: step 454900, loss = 1243266.00 (24.4 examples/sec; 0.656 sec/batch, epoch 98)\n",
      "2020-02-18 13:51:03.240859: step 455000, loss = 1483873.00 (22.6 examples/sec; 0.708 sec/batch, epoch 98)\n",
      "Doing validation\n",
      "validation loss: 5266235.828\n",
      "2020-02-18 13:52:19.265376: step 455100, loss = 1441216.00 (23.9 examples/sec; 0.669 sec/batch, epoch 99)\n",
      "2020-02-18 13:53:31.686021: step 455200, loss = 1273108.50 (23.9 examples/sec; 0.669 sec/batch, epoch 99)\n",
      "Doing validation\n",
      "validation loss: 5299662.914\n",
      "2020-02-18 13:54:53.422315: step 455300, loss = 1233862.00 (23.3 examples/sec; 0.687 sec/batch, epoch 100)\n",
      "Doing validation\n",
      "validation loss: 5217190.793\n",
      "2020-02-18 13:56:12.712871: step 455400, loss = 1355901.00 (22.8 examples/sec; 0.703 sec/batch, epoch 101)\n",
      "2020-02-18 13:57:22.065518: step 455500, loss = 1377090.00 (23.2 examples/sec; 0.690 sec/batch, epoch 101)\n",
      "Doing validation\n",
      "validation loss: 5210566.672\n",
      "2020-02-18 13:58:41.214900: step 455600, loss = 1140925.50 (23.2 examples/sec; 0.690 sec/batch, epoch 102)\n",
      "Doing validation\n",
      "validation loss: 5284609.345\n",
      "2020-02-18 14:00:03.606296: step 455700, loss = 1719492.00 (23.3 examples/sec; 0.686 sec/batch, epoch 103)\n",
      "2020-02-18 14:01:13.673964: step 455800, loss = 911963.00 (23.0 examples/sec; 0.694 sec/batch, epoch 103)\n",
      "Doing validation\n",
      "validation loss: 5222147.897\n",
      "2020-02-18 14:02:34.754651: step 455900, loss = 1530961.00 (22.7 examples/sec; 0.705 sec/batch, epoch 104)\n",
      "Doing validation\n",
      "validation loss: 5225213.741\n",
      "2020-02-18 14:03:54.307614: step 456000, loss = 1367989.00 (21.7 examples/sec; 0.738 sec/batch, epoch 105)\n",
      "step:  456000 save model:  train456000model.ckpt\n",
      "2020-02-18 14:05:11.722749: step 456100, loss = 1300916.00 (23.0 examples/sec; 0.695 sec/batch, epoch 105)\n",
      "Doing validation\n",
      "validation loss: 5226505.293\n",
      "2020-02-18 14:06:31.332442: step 456200, loss = 1403799.00 (22.9 examples/sec; 0.700 sec/batch, epoch 106)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doing validation\n",
      "validation loss: 5271215.534\n",
      "2020-02-18 14:07:50.994026: step 456300, loss = 1093027.00 (23.2 examples/sec; 0.691 sec/batch, epoch 107)\n",
      "2020-02-18 14:09:01.412447: step 456400, loss = 1762834.00 (23.0 examples/sec; 0.697 sec/batch, epoch 107)\n",
      "Doing validation\n",
      "validation loss: 5181123.966\n",
      "2020-02-18 14:10:24.059294: step 456500, loss = 1272251.00 (22.1 examples/sec; 0.724 sec/batch, epoch 108)\n",
      "Doing validation\n",
      "validation loss: 5289301.810\n",
      "2020-02-18 14:11:45.858272: step 456600, loss = 1226929.50 (21.8 examples/sec; 0.734 sec/batch, epoch 109)\n",
      "2020-02-18 14:12:58.401861: step 456700, loss = 1302077.00 (22.3 examples/sec; 0.716 sec/batch, epoch 109)\n",
      "Doing validation\n",
      "validation loss: 5246909.690\n",
      "2020-02-18 14:14:21.759148: step 456800, loss = 1362764.00 (22.6 examples/sec; 0.709 sec/batch, epoch 110)\n",
      "Doing validation\n",
      "validation loss: 5248762.138\n",
      "2020-02-18 14:15:42.962297: step 456900, loss = 1023209.00 (23.6 examples/sec; 0.678 sec/batch, epoch 111)\n",
      "2020-02-18 14:16:51.246979: step 457000, loss = 1382549.00 (24.2 examples/sec; 0.660 sec/batch, epoch 111)\n",
      "Doing validation\n",
      "validation loss: 5279054.414\n",
      "2020-02-18 14:18:07.561918: step 457100, loss = 1174902.00 (24.2 examples/sec; 0.660 sec/batch, epoch 112)\n",
      "Doing validation\n",
      "validation loss: 5198002.845\n",
      "2020-02-18 14:19:23.225631: step 457200, loss = 1665666.00 (23.7 examples/sec; 0.674 sec/batch, epoch 113)\n",
      "2020-02-18 14:20:33.462045: step 457300, loss = 1089208.50 (23.7 examples/sec; 0.676 sec/batch, epoch 113)\n",
      "Doing validation\n",
      "validation loss: 5347703.914\n",
      "2020-02-18 14:21:52.893649: step 457400, loss = 1310919.00 (23.8 examples/sec; 0.672 sec/batch, epoch 114)\n",
      "Doing validation\n",
      "validation loss: 5224097.793\n",
      "2020-02-18 14:23:12.554143: step 457500, loss = 941416.00 (23.8 examples/sec; 0.671 sec/batch, epoch 115)\n",
      "2020-02-18 14:24:23.848674: step 457600, loss = 1273604.00 (23.6 examples/sec; 0.677 sec/batch, epoch 115)\n",
      "Doing validation\n",
      "validation loss: 5313309.500\n",
      "2020-02-18 14:25:45.049627: step 457700, loss = 1344794.50 (23.5 examples/sec; 0.682 sec/batch, epoch 116)\n",
      "Doing validation\n",
      "validation loss: 5170410.483\n",
      "2020-02-18 14:27:02.848183: step 457800, loss = 1832974.00 (20.8 examples/sec; 0.769 sec/batch, epoch 117)\n",
      "2020-02-18 14:28:13.263160: step 457900, loss = 1079433.50 (22.4 examples/sec; 0.714 sec/batch, epoch 117)\n",
      "Doing validation\n",
      "validation loss: 5260667.879\n",
      "2020-02-18 14:29:33.776873: step 458000, loss = 1710511.00 (20.4 examples/sec; 0.786 sec/batch, epoch 118)\n",
      "Doing validation\n",
      "validation loss: 5296698.931\n",
      "2020-02-18 14:30:53.052287: step 458100, loss = 983628.50 (22.4 examples/sec; 0.715 sec/batch, epoch 119)\n",
      "2020-02-18 14:32:03.271969: step 458200, loss = 1317929.00 (23.8 examples/sec; 0.672 sec/batch, epoch 119)\n",
      "Doing validation\n",
      "validation loss: 5210107.138\n",
      "2020-02-18 14:33:22.317559: step 458300, loss = 1393903.50 (23.8 examples/sec; 0.673 sec/batch, epoch 120)\n",
      "Doing validation\n",
      "validation loss: 5218927.828\n",
      "2020-02-18 14:34:40.860170: step 458400, loss = 1033125.00 (23.4 examples/sec; 0.684 sec/batch, epoch 121)\n",
      "2020-02-18 14:35:48.979814: step 458500, loss = 1325013.00 (23.9 examples/sec; 0.668 sec/batch, epoch 121)\n",
      "Doing validation\n",
      "validation loss: 5279619.362\n",
      "2020-02-18 14:37:06.952764: step 458600, loss = 898907.00 (23.8 examples/sec; 0.673 sec/batch, epoch 122)\n",
      "Doing validation\n",
      "validation loss: 5283301.397\n",
      "2020-02-18 14:38:26.866208: step 458700, loss = 1022209.00 (19.7 examples/sec; 0.811 sec/batch, epoch 123)\n",
      "2020-02-18 14:39:36.660585: step 458800, loss = 1217384.00 (23.4 examples/sec; 0.683 sec/batch, epoch 123)\n",
      "Doing validation\n",
      "validation loss: 5260966.638\n",
      "2020-02-18 14:40:55.312404: step 458900, loss = 1240329.50 (24.0 examples/sec; 0.666 sec/batch, epoch 124)\n",
      "2020-02-18 14:42:03.164660: step 459000, loss = 1334793.50 (23.0 examples/sec; 0.697 sec/batch, epoch 124)\n",
      "Doing validation\n",
      "validation loss: 5230399.138\n",
      "2020-02-18 14:43:19.676079: step 459100, loss = 1571743.00 (22.9 examples/sec; 0.699 sec/batch, epoch 125)\n",
      "Doing validation\n",
      "validation loss: 5279615.207\n",
      "2020-02-18 14:44:35.632834: step 459200, loss = 1201486.50 (24.1 examples/sec; 0.665 sec/batch, epoch 126)\n",
      "2020-02-18 14:45:42.832436: step 459300, loss = 1118328.00 (24.2 examples/sec; 0.662 sec/batch, epoch 126)\n",
      "Doing validation\n",
      "validation loss: 5219742.483\n",
      "2020-02-18 14:46:58.928959: step 459400, loss = 1569938.00 (23.9 examples/sec; 0.668 sec/batch, epoch 127)\n",
      "Doing validation\n",
      "validation loss: 5247232.466\n",
      "2020-02-18 14:48:14.912605: step 459500, loss = 1445535.00 (24.1 examples/sec; 0.665 sec/batch, epoch 128)\n",
      "2020-02-18 14:49:21.943673: step 459600, loss = 1433699.50 (23.9 examples/sec; 0.670 sec/batch, epoch 128)\n",
      "Doing validation\n",
      "validation loss: 5279520.483\n",
      "2020-02-18 14:50:37.954522: step 459700, loss = 1409590.00 (24.0 examples/sec; 0.666 sec/batch, epoch 129)\n",
      "Doing validation\n",
      "validation loss: 5256226.828\n",
      "2020-02-18 14:51:55.075753: step 459800, loss = 1201586.50 (23.8 examples/sec; 0.672 sec/batch, epoch 130)\n",
      "2020-02-18 14:53:39.181900: step 459900, loss = 1034463.50 (9.3 examples/sec; 1.717 sec/batch, epoch 130)\n",
      "Doing validation\n",
      "validation loss: 5210972.241\n",
      "2020-02-18 14:55:29.131900: step 460000, loss = 1410573.00 (17.4 examples/sec; 0.920 sec/batch, epoch 131)\n",
      "step:  460000 save model:  train460000model.ckpt\n",
      "Doing validation\n",
      "validation loss: 5269081.086\n",
      "2020-02-18 14:57:20.550803: step 460100, loss = 1301060.00 (17.3 examples/sec; 0.927 sec/batch, epoch 132)\n",
      "2020-02-18 14:59:03.855991: step 460200, loss = 1685996.00 (14.9 examples/sec; 1.075 sec/batch, epoch 132)\n",
      "Doing validation\n",
      "validation loss: 5277698.603\n",
      "2020-02-18 15:01:10.248574: step 460300, loss = 1083552.50 (10.3 examples/sec; 1.561 sec/batch, epoch 133)\n",
      "Doing validation\n",
      "validation loss: 5209804.121\n",
      "2020-02-18 15:03:25.835672: step 460400, loss = 1551523.00 (14.5 examples/sec; 1.104 sec/batch, epoch 134)\n",
      "2020-02-18 15:05:18.538850: step 460500, loss = 1217751.00 (15.2 examples/sec; 1.049 sec/batch, epoch 134)\n",
      "Doing validation\n",
      "validation loss: 5286222.862\n",
      "2020-02-18 15:07:25.455487: step 460600, loss = 1040583.00 (9.5 examples/sec; 1.692 sec/batch, epoch 135)\n",
      "Doing validation\n",
      "validation loss: 5231457.621\n",
      "2020-02-18 15:09:30.198774: step 460700, loss = 1239214.00 (15.9 examples/sec; 1.003 sec/batch, epoch 136)\n",
      "2020-02-18 15:11:17.379268: step 460800, loss = 1456397.00 (17.5 examples/sec; 0.915 sec/batch, epoch 136)\n",
      "Doing validation\n",
      "validation loss: 5167609.862\n",
      "2020-02-18 15:13:29.265693: step 460900, loss = 1199121.50 (17.0 examples/sec; 0.943 sec/batch, epoch 137)\n",
      "Doing validation\n",
      "validation loss: 5178658.362\n",
      "2020-02-18 15:15:39.483519: step 461000, loss = 1108267.50 (14.9 examples/sec; 1.075 sec/batch, epoch 138)\n",
      "2020-02-18 15:17:32.029549: step 461100, loss = 1044870.50 (15.2 examples/sec; 1.049 sec/batch, epoch 138)\n",
      "Doing validation\n",
      "validation loss: 5261276.603\n",
      "2020-02-18 15:19:43.660349: step 461200, loss = 1241999.00 (11.5 examples/sec; 1.394 sec/batch, epoch 139)\n",
      "Doing validation\n",
      "validation loss: 5234066.138\n",
      "2020-02-18 15:21:53.688985: step 461300, loss = 1076856.50 (15.1 examples/sec; 1.062 sec/batch, epoch 140)\n",
      "2020-02-18 15:23:38.501984: step 461400, loss = 1157342.00 (17.1 examples/sec; 0.935 sec/batch, epoch 140)\n",
      "Doing validation\n",
      "validation loss: 5281738.138\n",
      "2020-02-18 15:25:32.119109: step 461500, loss = 1211699.00 (16.6 examples/sec; 0.966 sec/batch, epoch 141)\n",
      "Doing validation\n",
      "validation loss: 5227906.983\n",
      "2020-02-18 15:27:39.597400: step 461600, loss = 1480311.00 (16.3 examples/sec; 0.981 sec/batch, epoch 142)\n",
      "2020-02-18 15:29:24.354005: step 461700, loss = 1556115.00 (16.0 examples/sec; 1.002 sec/batch, epoch 142)\n",
      "Doing validation\n",
      "validation loss: 5185584.069\n",
      "2020-02-18 15:31:21.003545: step 461800, loss = 1333081.50 (18.4 examples/sec; 0.872 sec/batch, epoch 143)\n",
      "Doing validation\n",
      "validation loss: 5283579.724\n",
      "2020-02-18 15:32:58.272454: step 461900, loss = 1232203.00 (18.7 examples/sec; 0.858 sec/batch, epoch 144)\n",
      "2020-02-18 15:34:24.083964: step 462000, loss = 1429476.00 (18.6 examples/sec; 0.859 sec/batch, epoch 144)\n",
      "Doing validation\n",
      "validation loss: 5222519.707\n",
      "2020-02-18 15:36:01.728139: step 462100, loss = 1210720.00 (18.7 examples/sec; 0.857 sec/batch, epoch 145)\n",
      "Doing validation\n",
      "validation loss: 5274292.345\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-02-18 15:37:39.038991: step 462200, loss = 1008576.00 (18.7 examples/sec; 0.857 sec/batch, epoch 146)\n",
      "2020-02-18 15:39:04.574274: step 462300, loss = 1448372.00 (18.7 examples/sec; 0.855 sec/batch, epoch 146)\n",
      "Doing validation\n",
      "validation loss: 5239467.397\n",
      "2020-02-18 15:40:42.188300: step 462400, loss = 1022143.00 (18.4 examples/sec; 0.868 sec/batch, epoch 147)\n",
      "Doing validation\n",
      "validation loss: 5260297.172\n",
      "2020-02-18 15:42:19.613784: step 462500, loss = 1013958.00 (18.4 examples/sec; 0.868 sec/batch, epoch 148)\n",
      "2020-02-18 15:43:44.909674: step 462600, loss = 1276712.00 (18.5 examples/sec; 0.864 sec/batch, epoch 148)\n",
      "Doing validation\n",
      "validation loss: 5213683.414\n",
      "2020-02-18 15:45:23.623258: step 462700, loss = 1447262.00 (18.6 examples/sec; 0.859 sec/batch, epoch 149)\n",
      "2020-02-18 15:46:48.921174: step 462800, loss = 1259020.00 (18.6 examples/sec; 0.862 sec/batch, epoch 149)\n",
      "Doing validation\n",
      "validation loss: 5286733.397\n",
      "2020-02-18 15:48:26.330707: step 462900, loss = 1214597.00 (18.7 examples/sec; 0.856 sec/batch, epoch 150)\n",
      "Doing validation\n",
      "validation loss: 5335411.397\n",
      "2020-02-18 15:50:03.860919: step 463000, loss = 1332738.00 (18.5 examples/sec; 0.867 sec/batch, epoch 151)\n",
      "2020-02-18 15:51:29.317125: step 463100, loss = 1417866.00 (18.7 examples/sec; 0.854 sec/batch, epoch 151)\n",
      "Doing validation\n",
      "validation loss: 5203624.931\n",
      "2020-02-18 15:53:06.797257: step 463200, loss = 1247837.00 (18.9 examples/sec; 0.845 sec/batch, epoch 152)\n",
      "Doing validation\n",
      "validation loss: 5217745.431\n",
      "2020-02-18 15:54:44.412754: step 463300, loss = 1398355.00 (18.0 examples/sec; 0.889 sec/batch, epoch 153)\n",
      "2020-02-18 15:56:09.968490: step 463400, loss = 1139922.50 (18.8 examples/sec; 0.850 sec/batch, epoch 153)\n",
      "Doing validation\n",
      "validation loss: 5276884.707\n",
      "2020-02-18 15:57:47.688162: step 463500, loss = 1566270.50 (18.6 examples/sec; 0.862 sec/batch, epoch 154)\n",
      "Doing validation\n",
      "validation loss: 5280669.948\n",
      "2020-02-18 15:59:22.679548: step 463600, loss = 1039383.00 (24.1 examples/sec; 0.663 sec/batch, epoch 155)\n",
      "2020-02-18 16:00:29.774175: step 463700, loss = 1362282.00 (24.5 examples/sec; 0.654 sec/batch, epoch 155)\n",
      "Doing validation\n",
      "validation loss: 5241275.638\n",
      "2020-02-18 16:01:45.871097: step 463800, loss = 1144552.00 (23.8 examples/sec; 0.672 sec/batch, epoch 156)\n",
      "Doing validation\n",
      "validation loss: 5285294.966\n",
      "2020-02-18 16:03:02.134204: step 463900, loss = 1441412.50 (23.9 examples/sec; 0.668 sec/batch, epoch 157)\n",
      "2020-02-18 16:04:09.535993: step 464000, loss = 1564584.50 (23.8 examples/sec; 0.671 sec/batch, epoch 157)\n",
      "WARNING:tensorflow:From C:\\Users\\91828\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\saver.py:960: remove_checkpoint (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use standard file APIs to delete files with this prefix.\n",
      "step:  464000 save model:  train464000model.ckpt\n",
      "Doing validation\n",
      "validation loss: 5284869.483\n",
      "2020-02-18 16:05:29.821969: step 464100, loss = 1111890.00 (24.1 examples/sec; 0.664 sec/batch, epoch 158)\n",
      "Doing validation\n",
      "validation loss: 5209993.466\n",
      "2020-02-18 16:06:51.134497: step 464200, loss = 1515491.00 (22.2 examples/sec; 0.722 sec/batch, epoch 159)\n",
      "2020-02-18 16:07:58.864389: step 464300, loss = 1054505.50 (21.2 examples/sec; 0.753 sec/batch, epoch 159)\n",
      "Doing validation\n",
      "validation loss: 5226090.190\n",
      "2020-02-18 16:09:14.491957: step 464400, loss = 1281824.00 (24.2 examples/sec; 0.660 sec/batch, epoch 160)\n",
      "Doing validation\n",
      "validation loss: 5229611.293\n",
      "2020-02-18 16:10:30.616555: step 464500, loss = 1424775.00 (24.5 examples/sec; 0.654 sec/batch, epoch 161)\n",
      "2020-02-18 16:11:37.532340: step 464600, loss = 1172439.00 (23.9 examples/sec; 0.670 sec/batch, epoch 161)\n",
      "Doing validation\n",
      "validation loss: 5284571.086\n",
      "2020-02-18 16:12:53.280676: step 464700, loss = 1276483.50 (24.2 examples/sec; 0.660 sec/batch, epoch 162)\n",
      "Doing validation\n",
      "validation loss: 5274694.897\n",
      "2020-02-18 16:14:08.954203: step 464800, loss = 1314078.00 (23.7 examples/sec; 0.676 sec/batch, epoch 163)\n",
      "2020-02-18 16:15:15.937172: step 464900, loss = 1263020.00 (23.8 examples/sec; 0.673 sec/batch, epoch 163)\n",
      "Doing validation\n",
      "validation loss: 5239638.638\n",
      "2020-02-18 16:16:31.910860: step 465000, loss = 1105263.00 (24.1 examples/sec; 0.665 sec/batch, epoch 164)\n",
      "Doing validation\n",
      "validation loss: 5245447.845\n",
      "2020-02-18 16:17:59.863204: step 465100, loss = 1180757.00 (17.4 examples/sec; 0.918 sec/batch, epoch 165)\n",
      "2020-02-18 18:10:30.499401: step 465200, loss = 951697.50 (14.2 examples/sec; 1.128 sec/batch, epoch 165)\n",
      "Doing validation\n",
      "validation loss: 5213227.448\n",
      "2020-02-18 18:12:27.426264: step 465300, loss = 1572526.00 (17.0 examples/sec; 0.939 sec/batch, epoch 166)\n",
      "Doing validation\n",
      "validation loss: 5299235.069\n",
      "2020-02-18 18:14:27.429449: step 465400, loss = 1684257.00 (17.3 examples/sec; 0.924 sec/batch, epoch 167)\n",
      "2020-02-18 18:16:15.270494: step 465500, loss = 1095043.00 (16.8 examples/sec; 0.951 sec/batch, epoch 167)\n",
      "Doing validation\n",
      "validation loss: 5276805.897\n",
      "2020-02-18 18:18:10.497147: step 465600, loss = 1257476.00 (17.3 examples/sec; 0.928 sec/batch, epoch 168)\n",
      "Doing validation\n",
      "validation loss: 5271505.534\n",
      "2020-02-18 18:20:16.028704: step 465700, loss = 1704341.00 (16.9 examples/sec; 0.949 sec/batch, epoch 169)\n",
      "2020-02-18 18:21:54.418568: step 465800, loss = 1369035.00 (19.2 examples/sec; 0.835 sec/batch, epoch 169)\n",
      "Doing validation\n",
      "validation loss: 5252602.603\n",
      "2020-02-18 18:23:30.356721: step 465900, loss = 1194571.50 (19.4 examples/sec; 0.824 sec/batch, epoch 170)\n",
      "Doing validation\n",
      "validation loss: 5252613.810\n",
      "2020-02-18 18:25:08.771526: step 466000, loss = 1292111.00 (19.0 examples/sec; 0.841 sec/batch, epoch 171)\n",
      "2020-02-18 18:26:33.643283: step 466100, loss = 1320026.00 (18.6 examples/sec; 0.860 sec/batch, epoch 171)\n",
      "Doing validation\n",
      "validation loss: 5273991.517\n",
      "2020-02-18 18:28:09.959738: step 466200, loss = 1513746.00 (18.9 examples/sec; 0.848 sec/batch, epoch 172)\n",
      "Doing validation\n",
      "validation loss: 5291422.328\n",
      "2020-02-18 18:29:46.540977: step 466300, loss = 1466143.00 (19.6 examples/sec; 0.818 sec/batch, epoch 173)\n",
      "2020-02-18 18:31:10.875471: step 466400, loss = 1391856.00 (19.2 examples/sec; 0.834 sec/batch, epoch 173)\n",
      "Doing validation\n",
      "validation loss: 5251592.845\n",
      "2020-02-18 18:32:47.316413: step 466500, loss = 1245811.50 (18.5 examples/sec; 0.864 sec/batch, epoch 174)\n",
      "2020-02-18 18:34:12.319151: step 466600, loss = 1454563.00 (19.3 examples/sec; 0.830 sec/batch, epoch 174)\n",
      "Doing validation\n",
      "validation loss: 5196356.172\n",
      "2020-02-18 18:35:48.193854: step 466700, loss = 1134472.00 (19.1 examples/sec; 0.839 sec/batch, epoch 175)\n",
      "Doing validation\n",
      "validation loss: 5305905.569\n",
      "2020-02-18 18:37:25.510719: step 466800, loss = 1070429.00 (19.1 examples/sec; 0.840 sec/batch, epoch 176)\n",
      "2020-02-18 18:38:49.961900: step 466900, loss = 1390210.00 (18.7 examples/sec; 0.854 sec/batch, epoch 176)\n",
      "Doing validation\n",
      "validation loss: 5192412.328\n",
      "2020-02-18 18:40:19.487475: step 467000, loss = 1412769.50 (22.8 examples/sec; 0.701 sec/batch, epoch 177)\n",
      "Doing validation\n",
      "validation loss: 5221012.138\n",
      "2020-02-18 18:41:38.940186: step 467100, loss = 1382626.50 (23.0 examples/sec; 0.696 sec/batch, epoch 178)\n",
      "2020-02-18 18:42:48.796640: step 467200, loss = 1111913.00 (22.7 examples/sec; 0.704 sec/batch, epoch 178)\n",
      "Doing validation\n",
      "validation loss: 5283271.569\n",
      "2020-02-18 18:44:08.332899: step 467300, loss = 1355230.50 (23.1 examples/sec; 0.693 sec/batch, epoch 179)\n",
      "Doing validation\n",
      "validation loss: 5302612.983\n",
      "2020-02-18 18:45:28.034782: step 467400, loss = 1296361.50 (23.0 examples/sec; 0.695 sec/batch, epoch 180)\n",
      "2020-02-18 18:46:37.873043: step 467500, loss = 1189698.50 (23.1 examples/sec; 0.691 sec/batch, epoch 180)\n",
      "Doing validation\n",
      "validation loss: 5228832.707\n",
      "2020-02-18 18:47:57.372245: step 467600, loss = 1162007.00 (23.0 examples/sec; 0.695 sec/batch, epoch 181)\n",
      "Doing validation\n",
      "validation loss: 5240410.500\n",
      "2020-02-18 18:49:16.898597: step 467700, loss = 1563962.00 (22.9 examples/sec; 0.699 sec/batch, epoch 182)\n",
      "2020-02-18 18:50:26.860523: step 467800, loss = 1383042.00 (22.9 examples/sec; 0.698 sec/batch, epoch 182)\n",
      "Doing validation\n",
      "validation loss: 5273070.293\n",
      "2020-02-18 18:51:46.476826: step 467900, loss = 1045203.00 (22.9 examples/sec; 0.700 sec/batch, epoch 183)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doing validation\n",
      "validation loss: 5180659.655\n",
      "2020-02-18 18:53:07.808348: step 468000, loss = 1298192.00 (23.0 examples/sec; 0.695 sec/batch, epoch 184)\n",
      "step:  468000 save model:  train468000model.ckpt\n",
      "2020-02-18 18:54:22.073377: step 468100, loss = 1126358.00 (22.9 examples/sec; 0.700 sec/batch, epoch 184)\n",
      "Doing validation\n",
      "validation loss: 5294618.724\n",
      "2020-02-18 18:55:41.712396: step 468200, loss = 1358230.00 (22.9 examples/sec; 0.700 sec/batch, epoch 185)\n",
      "Doing validation\n",
      "validation loss: 5239980.966\n",
      "2020-02-18 18:57:01.193719: step 468300, loss = 1291491.50 (23.2 examples/sec; 0.690 sec/batch, epoch 186)\n",
      "2020-02-18 18:58:10.940063: step 468400, loss = 1160469.00 (23.2 examples/sec; 0.689 sec/batch, epoch 186)\n",
      "Doing validation\n",
      "validation loss: 5241100.914\n",
      "2020-02-18 18:59:30.632101: step 468500, loss = 1211928.00 (22.7 examples/sec; 0.706 sec/batch, epoch 187)\n",
      "Doing validation\n",
      "validation loss: 5259003.862\n",
      "2020-02-18 19:00:50.296123: step 468600, loss = 1383826.00 (22.8 examples/sec; 0.701 sec/batch, epoch 188)\n",
      "2020-02-18 19:02:00.225144: step 468700, loss = 1409814.00 (22.9 examples/sec; 0.699 sec/batch, epoch 188)\n",
      "Doing validation\n",
      "validation loss: 5242641.190\n",
      "2020-02-18 19:03:19.647758: step 468800, loss = 1136145.50 (22.8 examples/sec; 0.703 sec/batch, epoch 189)\n",
      "Doing validation\n",
      "validation loss: 5240932.966\n",
      "2020-02-18 19:04:39.113940: step 468900, loss = 1143253.00 (23.2 examples/sec; 0.690 sec/batch, epoch 190)\n",
      "2020-02-18 19:05:49.212254: step 469000, loss = 1299088.00 (22.6 examples/sec; 0.708 sec/batch, epoch 190)\n",
      "Doing validation\n",
      "validation loss: 5251651.810\n",
      "2020-02-18 19:07:08.632888: step 469100, loss = 1138297.50 (23.1 examples/sec; 0.693 sec/batch, epoch 191)\n",
      "Doing validation\n",
      "validation loss: 5213945.569\n",
      "2020-02-18 19:08:28.222072: step 469200, loss = 1389910.00 (22.8 examples/sec; 0.703 sec/batch, epoch 192)\n",
      "2020-02-18 19:09:38.179938: step 469300, loss = 1456543.00 (22.7 examples/sec; 0.704 sec/batch, epoch 192)\n",
      "Doing validation\n",
      "validation loss: 5269083.207\n",
      "2020-02-18 19:10:58.039481: step 469400, loss = 1351192.50 (23.1 examples/sec; 0.694 sec/batch, epoch 193)\n",
      "Doing validation\n",
      "validation loss: 5296919.983\n",
      "2020-02-18 19:12:18.849751: step 469500, loss = 1455107.00 (22.7 examples/sec; 0.705 sec/batch, epoch 194)\n",
      "2020-02-18 19:13:29.239271: step 469600, loss = 1296102.50 (23.1 examples/sec; 0.693 sec/batch, epoch 194)\n",
      "Doing validation\n",
      "validation loss: 5257282.138\n",
      "2020-02-18 19:14:51.232031: step 469700, loss = 1305172.00 (22.7 examples/sec; 0.704 sec/batch, epoch 195)\n",
      "Doing validation\n",
      "validation loss: 5228918.897\n",
      "2020-02-18 19:16:13.234768: step 469800, loss = 1109047.00 (21.8 examples/sec; 0.734 sec/batch, epoch 196)\n",
      "2020-02-18 19:17:24.553057: step 469900, loss = 1500653.00 (23.9 examples/sec; 0.670 sec/batch, epoch 196)\n",
      "Doing validation\n",
      "validation loss: 5311937.862\n",
      "2020-02-18 19:18:42.245838: step 470000, loss = 999663.50 (24.2 examples/sec; 0.661 sec/batch, epoch 197)\n",
      "Doing validation\n",
      "validation loss: 5208973.397\n",
      "2020-02-18 19:20:01.597033: step 470100, loss = 1404548.50 (20.6 examples/sec; 0.776 sec/batch, epoch 198)\n",
      "2020-02-18 19:21:10.489080: step 470200, loss = 1247069.00 (24.1 examples/sec; 0.664 sec/batch, epoch 198)\n",
      "Doing validation\n",
      "validation loss: 5267709.897\n",
      "2020-02-18 19:22:30.760903: step 470300, loss = 1464343.00 (23.9 examples/sec; 0.670 sec/batch, epoch 199)\n",
      "2020-02-18 19:23:40.243474: step 470400, loss = 1220819.00 (18.9 examples/sec; 0.849 sec/batch, epoch 199)\n",
      "Doing validation\n",
      "validation loss: 5217302.293\n",
      "2020-02-18 19:24:58.060405: step 470500, loss = 1486609.00 (23.8 examples/sec; 0.673 sec/batch, epoch 200)\n",
      "Doing validation\n",
      "validation loss: 5277317.259\n",
      "2020-02-18 19:26:17.376690: step 470600, loss = 1184586.00 (24.2 examples/sec; 0.661 sec/batch, epoch 201)\n",
      "2020-02-18 19:27:27.051384: step 470700, loss = 1320520.50 (23.9 examples/sec; 0.670 sec/batch, epoch 201)\n",
      "Doing validation\n",
      "validation loss: 5212492.638\n",
      "2020-02-18 19:28:46.151427: step 470800, loss = 1298404.00 (23.6 examples/sec; 0.677 sec/batch, epoch 202)\n",
      "Doing validation\n",
      "validation loss: 5296052.138\n",
      "2020-02-18 19:30:05.320733: step 470900, loss = 1152511.00 (23.5 examples/sec; 0.680 sec/batch, epoch 203)\n",
      "2020-02-18 19:31:15.930700: step 471000, loss = 1074691.00 (21.4 examples/sec; 0.747 sec/batch, epoch 203)\n",
      "Doing validation\n",
      "validation loss: 5191752.224\n",
      "2020-02-18 19:32:36.711581: step 471100, loss = 1527920.50 (23.4 examples/sec; 0.683 sec/batch, epoch 204)\n",
      "Doing validation\n",
      "validation loss: 5275931.500\n",
      "2020-02-18 19:33:55.293469: step 471200, loss = 1290393.00 (24.1 examples/sec; 0.663 sec/batch, epoch 205)\n",
      "2020-02-18 19:35:03.512871: step 471300, loss = 1471964.00 (24.1 examples/sec; 0.664 sec/batch, epoch 205)\n",
      "Doing validation\n",
      "validation loss: 5245784.724\n",
      "2020-02-18 19:36:21.401083: step 471400, loss = 1079471.50 (20.9 examples/sec; 0.764 sec/batch, epoch 206)\n",
      "Doing validation\n",
      "validation loss: 5231294.414\n",
      "2020-02-18 19:37:38.971230: step 471500, loss = 1240082.00 (20.4 examples/sec; 0.785 sec/batch, epoch 207)\n",
      "2020-02-18 19:38:48.420528: step 471600, loss = 1050781.50 (23.7 examples/sec; 0.674 sec/batch, epoch 207)\n",
      "Doing validation\n",
      "validation loss: 5261378.483\n",
      "2020-02-18 19:40:08.727287: step 471700, loss = 1362849.50 (23.7 examples/sec; 0.674 sec/batch, epoch 208)\n",
      "Doing validation\n",
      "validation loss: 5277316.862\n",
      "2020-02-18 19:41:29.736413: step 471800, loss = 1351610.50 (23.7 examples/sec; 0.675 sec/batch, epoch 209)\n",
      "2020-02-18 19:42:39.515702: step 471900, loss = 1195265.00 (22.6 examples/sec; 0.707 sec/batch, epoch 209)\n",
      "Doing validation\n",
      "validation loss: 5219972.483\n",
      "2020-02-18 19:44:01.361262: step 472000, loss = 1221247.50 (22.6 examples/sec; 0.708 sec/batch, epoch 210)\n",
      "step:  472000 save model:  train472000model.ckpt\n",
      "Doing validation\n",
      "validation loss: 5239213.345\n",
      "2020-02-18 19:45:29.701431: step 472100, loss = 1098804.50 (23.0 examples/sec; 0.696 sec/batch, epoch 211)\n",
      "2020-02-18 19:46:40.065866: step 472200, loss = 1281161.00 (19.7 examples/sec; 0.814 sec/batch, epoch 211)\n",
      "Doing validation\n",
      "validation loss: 5226965.466\n",
      "2020-02-18 19:48:01.652655: step 472300, loss = 1118154.00 (21.7 examples/sec; 0.738 sec/batch, epoch 212)\n",
      "Doing validation\n",
      "validation loss: 5307651.569\n",
      "2020-02-18 19:49:24.081293: step 472400, loss = 1413289.00 (23.5 examples/sec; 0.682 sec/batch, epoch 213)\n",
      "2020-02-18 19:50:39.403463: step 472500, loss = 1346474.50 (22.0 examples/sec; 0.726 sec/batch, epoch 213)\n",
      "Doing validation\n",
      "validation loss: 5280608.707\n",
      "2020-02-18 19:52:00.145788: step 472600, loss = 1065521.50 (23.0 examples/sec; 0.695 sec/batch, epoch 214)\n",
      "Doing validation\n",
      "validation loss: 5257989.000\n",
      "2020-02-18 19:53:20.979687: step 472700, loss = 984493.00 (24.2 examples/sec; 0.662 sec/batch, epoch 215)\n",
      "2020-02-18 19:54:32.538205: step 472800, loss = 1606912.00 (20.5 examples/sec; 0.781 sec/batch, epoch 215)\n",
      "Doing validation\n",
      "validation loss: 5215405.293\n",
      "2020-02-18 19:55:56.962798: step 472900, loss = 1090766.00 (23.0 examples/sec; 0.695 sec/batch, epoch 216)\n",
      "Doing validation\n",
      "validation loss: 5318155.207\n",
      "2020-02-18 19:57:20.186264: step 473000, loss = 1222717.00 (24.0 examples/sec; 0.667 sec/batch, epoch 217)\n",
      "2020-02-18 19:58:29.319456: step 473100, loss = 1188135.50 (20.7 examples/sec; 0.772 sec/batch, epoch 217)\n",
      "Doing validation\n",
      "validation loss: 5246987.776\n",
      "2020-02-18 19:59:51.510634: step 473200, loss = 1131338.00 (20.5 examples/sec; 0.780 sec/batch, epoch 218)\n",
      "Doing validation\n",
      "validation loss: 5279856.483\n",
      "2020-02-18 20:01:11.162697: step 473300, loss = 1105910.00 (23.6 examples/sec; 0.678 sec/batch, epoch 219)\n",
      "2020-02-18 20:02:20.596556: step 473400, loss = 1133376.50 (20.9 examples/sec; 0.765 sec/batch, epoch 219)\n",
      "Doing validation\n",
      "validation loss: 5257123.483\n",
      "2020-02-18 20:03:42.172424: step 473500, loss = 1257689.00 (18.4 examples/sec; 0.871 sec/batch, epoch 220)\n",
      "Doing validation\n",
      "validation loss: 5255288.241\n",
      "2020-02-18 20:05:04.040817: step 473600, loss = 937734.50 (23.3 examples/sec; 0.686 sec/batch, epoch 221)\n",
      "2020-02-18 20:06:15.977723: step 473700, loss = 1301579.00 (23.5 examples/sec; 0.681 sec/batch, epoch 221)\n",
      "Doing validation\n",
      "validation loss: 5225102.293\n",
      "2020-02-18 20:07:36.220638: step 473800, loss = 1292987.00 (24.1 examples/sec; 0.665 sec/batch, epoch 222)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doing validation\n",
      "validation loss: 5181111.241\n",
      "2020-02-18 20:09:37.795861: step 473900, loss = 1532977.00 (15.5 examples/sec; 1.030 sec/batch, epoch 223)\n",
      "2020-02-18 20:11:35.381224: step 474000, loss = 1296577.00 (8.0 examples/sec; 2.005 sec/batch, epoch 223)\n",
      "Doing validation\n",
      "validation loss: 5324766.017\n",
      "2020-02-18 20:13:38.822794: step 474100, loss = 1020729.00 (14.1 examples/sec; 1.137 sec/batch, epoch 224)\n",
      "2020-02-18 20:15:34.754171: step 474200, loss = 1319192.00 (13.7 examples/sec; 1.167 sec/batch, epoch 224)\n",
      "Doing validation\n",
      "validation loss: 5231594.707\n",
      "2020-02-18 20:17:38.064468: step 474300, loss = 1063732.00 (16.7 examples/sec; 0.958 sec/batch, epoch 225)\n",
      "Doing validation\n",
      "validation loss: 5275605.621\n",
      "2020-02-18 20:19:38.211524: step 474400, loss = 1160806.50 (15.8 examples/sec; 1.013 sec/batch, epoch 226)\n",
      "2020-02-18 20:21:23.645751: step 474500, loss = 1105497.00 (16.4 examples/sec; 0.977 sec/batch, epoch 226)\n",
      "Doing validation\n",
      "validation loss: 5268291.172\n",
      "2020-02-18 20:23:23.359442: step 474600, loss = 1037659.00 (16.5 examples/sec; 0.972 sec/batch, epoch 227)\n",
      "Doing validation\n",
      "validation loss: 5224968.397\n",
      "2020-02-18 20:25:18.248940: step 474700, loss = 1632891.00 (16.0 examples/sec; 0.999 sec/batch, epoch 228)\n",
      "2020-02-18 20:26:49.273566: step 474800, loss = 1060311.00 (17.3 examples/sec; 0.925 sec/batch, epoch 228)\n",
      "Doing validation\n",
      "validation loss: 5219517.621\n",
      "2020-02-18 20:28:33.267492: step 474900, loss = 1088136.50 (17.4 examples/sec; 0.918 sec/batch, epoch 229)\n",
      "Doing validation\n",
      "validation loss: 5254338.828\n",
      "2020-02-18 20:30:18.888111: step 475000, loss = 1281842.00 (17.2 examples/sec; 0.931 sec/batch, epoch 230)\n",
      "2020-02-18 20:31:51.157348: step 475100, loss = 1248290.00 (17.3 examples/sec; 0.924 sec/batch, epoch 230)\n",
      "Doing validation\n",
      "validation loss: 5274899.172\n",
      "2020-02-18 20:33:35.634001: step 475200, loss = 1375214.50 (15.4 examples/sec; 1.038 sec/batch, epoch 231)\n",
      "Doing validation\n",
      "validation loss: 5258290.983\n",
      "2020-02-18 20:35:27.745129: step 475300, loss = 1519874.00 (17.4 examples/sec; 0.921 sec/batch, epoch 232)\n",
      "2020-02-18 20:37:00.917991: step 475400, loss = 1370968.50 (17.3 examples/sec; 0.925 sec/batch, epoch 232)\n",
      "Doing validation\n",
      "validation loss: 5271032.259\n",
      "2020-02-18 20:38:44.970326: step 475500, loss = 1307262.00 (17.4 examples/sec; 0.920 sec/batch, epoch 233)\n",
      "Doing validation\n",
      "validation loss: 5250528.034\n",
      "2020-02-18 20:40:30.915039: step 475600, loss = 1646407.00 (17.8 examples/sec; 0.901 sec/batch, epoch 234)\n",
      "2020-02-18 20:42:03.867490: step 475700, loss = 1414811.50 (17.6 examples/sec; 0.910 sec/batch, epoch 234)\n",
      "Doing validation\n",
      "validation loss: 5226995.638\n",
      "2020-02-18 20:43:48.388997: step 475800, loss = 1381655.00 (17.2 examples/sec; 0.931 sec/batch, epoch 235)\n",
      "Doing validation\n",
      "validation loss: 5278649.793\n",
      "2020-02-18 20:45:34.792450: step 475900, loss = 1163666.00 (17.4 examples/sec; 0.921 sec/batch, epoch 236)\n",
      "2020-02-18 20:47:08.774148: step 476000, loss = 1137772.00 (16.0 examples/sec; 0.999 sec/batch, epoch 236)\n",
      "step:  476000 save model:  train476000model.ckpt\n",
      "Doing validation\n",
      "validation loss: 5264854.362\n",
      "2020-02-18 20:49:14.243906: step 476100, loss = 1165796.50 (18.9 examples/sec; 0.845 sec/batch, epoch 237)\n",
      "Doing validation\n",
      "validation loss: 5251911.276\n",
      "2020-02-18 20:50:50.375678: step 476200, loss = 1304292.00 (19.1 examples/sec; 0.840 sec/batch, epoch 238)\n",
      "2020-02-18 20:52:14.189040: step 476300, loss = 937273.00 (19.1 examples/sec; 0.838 sec/batch, epoch 238)\n",
      "Doing validation\n",
      "validation loss: 5223916.672\n",
      "2020-02-18 20:53:50.161405: step 476400, loss = 1057015.50 (18.9 examples/sec; 0.846 sec/batch, epoch 239)\n",
      "Doing validation\n",
      "validation loss: 5271244.914\n",
      "2020-02-18 20:55:26.112789: step 476500, loss = 1596931.00 (19.8 examples/sec; 0.807 sec/batch, epoch 240)\n",
      "2020-02-18 20:56:49.882791: step 476600, loss = 1468810.00 (19.8 examples/sec; 0.810 sec/batch, epoch 240)\n",
      "Doing validation\n",
      "validation loss: 5242263.328\n",
      "2020-02-18 20:58:25.958931: step 476700, loss = 1229009.00 (19.0 examples/sec; 0.842 sec/batch, epoch 241)\n",
      "Doing validation\n",
      "validation loss: 5272128.534\n",
      "2020-02-18 21:00:01.710781: step 476800, loss = 1217413.00 (18.9 examples/sec; 0.848 sec/batch, epoch 242)\n",
      "2020-02-18 21:01:25.658419: step 476900, loss = 1006082.00 (18.5 examples/sec; 0.867 sec/batch, epoch 242)\n",
      "Doing validation\n",
      "validation loss: 5195239.224\n",
      "2020-02-18 21:03:01.372484: step 477000, loss = 1172331.00 (19.1 examples/sec; 0.839 sec/batch, epoch 243)\n",
      "Doing validation\n",
      "validation loss: 5253767.121\n",
      "2020-02-18 21:04:37.325406: step 477100, loss = 1409935.00 (19.5 examples/sec; 0.822 sec/batch, epoch 244)\n",
      "2020-02-18 21:06:01.146262: step 477200, loss = 1475239.50 (18.6 examples/sec; 0.861 sec/batch, epoch 244)\n",
      "Doing validation\n",
      "validation loss: 5326509.241\n",
      "2020-02-18 21:07:36.761427: step 477300, loss = 1378150.00 (19.3 examples/sec; 0.831 sec/batch, epoch 245)\n",
      "Doing validation\n",
      "validation loss: 5231881.293\n",
      "2020-02-18 21:09:12.778684: step 477400, loss = 1221297.00 (18.7 examples/sec; 0.855 sec/batch, epoch 246)\n",
      "2020-02-18 21:10:37.958916: step 477500, loss = 1014837.00 (19.8 examples/sec; 0.808 sec/batch, epoch 246)\n",
      "Doing validation\n",
      "validation loss: 5279989.897\n",
      "2020-02-18 21:12:13.960173: step 477600, loss = 1208854.00 (19.7 examples/sec; 0.814 sec/batch, epoch 247)\n",
      "Doing validation\n",
      "validation loss: 5274607.517\n",
      "2020-02-18 21:13:50.725430: step 477700, loss = 1678974.00 (16.2 examples/sec; 0.985 sec/batch, epoch 248)\n",
      "2020-02-18 21:15:15.384057: step 477800, loss = 1432414.00 (19.1 examples/sec; 0.839 sec/batch, epoch 248)\n",
      "Doing validation\n",
      "validation loss: 5250458.155\n",
      "2020-02-18 21:16:51.192283: step 477900, loss = 1633662.00 (19.9 examples/sec; 0.804 sec/batch, epoch 249)\n",
      "2020-02-18 21:18:17.779105: step 478000, loss = 1115825.50 (18.9 examples/sec; 0.848 sec/batch, epoch 249)\n",
      "Doing validation\n",
      "validation loss: 5281681.397\n",
      "2020-02-18 21:19:53.820333: step 478100, loss = 1300966.50 (19.8 examples/sec; 0.809 sec/batch, epoch 250)\n",
      "Doing validation\n",
      "validation loss: 5267824.448\n",
      "2020-02-18 21:21:29.323838: step 478200, loss = 1238201.50 (19.1 examples/sec; 0.836 sec/batch, epoch 251)\n",
      "2020-02-18 21:22:44.572235: step 478300, loss = 1339019.50 (23.8 examples/sec; 0.672 sec/batch, epoch 251)\n",
      "Doing validation\n",
      "validation loss: 5203860.121\n",
      "2020-02-18 21:24:03.829304: step 478400, loss = 1048188.00 (23.6 examples/sec; 0.677 sec/batch, epoch 252)\n",
      "Doing validation\n",
      "validation loss: 5221060.845\n",
      "2020-02-18 21:25:21.006768: step 478500, loss = 1460025.00 (23.7 examples/sec; 0.674 sec/batch, epoch 253)\n",
      "2020-02-18 21:26:29.016891: step 478600, loss = 1067336.00 (23.4 examples/sec; 0.685 sec/batch, epoch 253)\n",
      "Doing validation\n",
      "validation loss: 5253458.034\n",
      "2020-02-18 21:27:48.403488: step 478700, loss = 1601496.00 (23.7 examples/sec; 0.675 sec/batch, epoch 254)\n",
      "Doing validation\n",
      "validation loss: 5244872.466\n",
      "2020-02-18 21:29:06.375058: step 478800, loss = 994782.50 (23.4 examples/sec; 0.683 sec/batch, epoch 255)\n",
      "2020-02-18 21:30:14.784090: step 478900, loss = 1114550.00 (23.5 examples/sec; 0.680 sec/batch, epoch 255)\n",
      "Doing validation\n",
      "validation loss: 5285800.224\n",
      "2020-02-18 21:31:32.070431: step 479000, loss = 1343860.00 (23.8 examples/sec; 0.672 sec/batch, epoch 256)\n",
      "Doing validation\n",
      "validation loss: 5250486.362\n",
      "2020-02-18 21:32:49.316604: step 479100, loss = 1367490.50 (23.6 examples/sec; 0.679 sec/batch, epoch 257)\n",
      "2020-02-18 21:33:57.331265: step 479200, loss = 1334854.00 (23.4 examples/sec; 0.683 sec/batch, epoch 257)\n",
      "Doing validation\n",
      "validation loss: 5255972.466\n",
      "2020-02-18 21:35:14.917805: step 479300, loss = 1486852.50 (23.8 examples/sec; 0.671 sec/batch, epoch 258)\n",
      "Doing validation\n",
      "validation loss: 5304258.362\n",
      "2020-02-18 21:36:32.367736: step 479400, loss = 1139414.50 (23.5 examples/sec; 0.681 sec/batch, epoch 259)\n",
      "2020-02-18 21:37:41.466643: step 479500, loss = 1295323.50 (22.3 examples/sec; 0.719 sec/batch, epoch 259)\n",
      "Doing validation\n",
      "validation loss: 5243910.431\n",
      "2020-02-18 21:38:58.852563: step 479600, loss = 1278817.50 (23.6 examples/sec; 0.679 sec/batch, epoch 260)\n",
      "Doing validation\n",
      "validation loss: 5238674.121\n",
      "2020-02-18 21:40:16.256631: step 479700, loss = 1329371.50 (23.6 examples/sec; 0.679 sec/batch, epoch 261)\n",
      "2020-02-18 21:41:24.450834: step 479800, loss = 1546382.00 (23.3 examples/sec; 0.687 sec/batch, epoch 261)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doing validation\n",
      "validation loss: 5219764.397\n",
      "2020-02-18 21:42:41.796647: step 479900, loss = 1152811.50 (23.2 examples/sec; 0.689 sec/batch, epoch 262)\n",
      "Doing validation\n",
      "validation loss: 5272149.121\n",
      "2020-02-18 21:43:59.105768: step 480000, loss = 1419975.00 (23.7 examples/sec; 0.674 sec/batch, epoch 263)\n",
      "step:  480000 save model:  train480000model.ckpt\n",
      "2020-02-18 21:45:10.206341: step 480100, loss = 1379899.00 (23.6 examples/sec; 0.679 sec/batch, epoch 263)\n",
      "Doing validation\n",
      "validation loss: 5240731.362\n",
      "2020-02-18 21:46:27.476165: step 480200, loss = 1589090.00 (23.3 examples/sec; 0.687 sec/batch, epoch 264)\n",
      "Doing validation\n",
      "validation loss: 5243627.828\n",
      "2020-02-18 21:47:44.860531: step 480300, loss = 1287934.50 (23.4 examples/sec; 0.684 sec/batch, epoch 265)\n",
      "2020-02-18 21:48:53.045400: step 480400, loss = 1416690.50 (23.8 examples/sec; 0.672 sec/batch, epoch 265)\n",
      "Doing validation\n",
      "validation loss: 5244751.310\n",
      "2020-02-18 21:50:10.508270: step 480500, loss = 1058832.00 (23.5 examples/sec; 0.681 sec/batch, epoch 266)\n",
      "Doing validation\n",
      "validation loss: 5315314.586\n",
      "2020-02-18 21:51:27.786171: step 480600, loss = 1434843.50 (23.1 examples/sec; 0.692 sec/batch, epoch 267)\n",
      "2020-02-18 21:52:35.829230: step 480700, loss = 1323440.00 (23.6 examples/sec; 0.677 sec/batch, epoch 267)\n",
      "Doing validation\n",
      "validation loss: 5257759.500\n",
      "2020-02-18 21:53:53.168005: step 480800, loss = 1170989.50 (23.5 examples/sec; 0.682 sec/batch, epoch 268)\n",
      "Doing validation\n",
      "validation loss: 5212957.724\n",
      "2020-02-18 21:55:10.651849: step 480900, loss = 1665637.00 (23.5 examples/sec; 0.680 sec/batch, epoch 269)\n",
      "2020-02-18 21:56:18.611048: step 481000, loss = 1489484.00 (23.5 examples/sec; 0.682 sec/batch, epoch 269)\n",
      "Doing validation\n",
      "validation loss: 5219392.397\n",
      "2020-02-18 21:57:36.129253: step 481100, loss = 1285088.00 (23.9 examples/sec; 0.670 sec/batch, epoch 270)\n",
      "Doing validation\n",
      "validation loss: 5280905.845\n",
      "2020-02-18 21:58:53.538265: step 481200, loss = 1355582.50 (23.3 examples/sec; 0.686 sec/batch, epoch 271)\n",
      "2020-02-18 22:00:01.642160: step 481300, loss = 1205861.00 (23.7 examples/sec; 0.674 sec/batch, epoch 271)\n",
      "Doing validation\n",
      "validation loss: 5253155.259\n",
      "2020-02-18 22:01:18.957423: step 481400, loss = 1108602.00 (23.6 examples/sec; 0.679 sec/batch, epoch 272)\n",
      "Doing validation\n",
      "validation loss: 5237868.828\n",
      "2020-02-18 22:02:36.266920: step 481500, loss = 1332789.00 (23.6 examples/sec; 0.678 sec/batch, epoch 273)\n",
      "2020-02-18 22:03:44.334072: step 481600, loss = 1382657.00 (23.5 examples/sec; 0.681 sec/batch, epoch 273)\n",
      "Doing validation\n",
      "validation loss: 5288187.397\n",
      "2020-02-18 22:05:01.759285: step 481700, loss = 1250435.00 (23.8 examples/sec; 0.673 sec/batch, epoch 274)\n",
      "2020-02-18 22:06:09.874104: step 481800, loss = 1442045.50 (23.7 examples/sec; 0.676 sec/batch, epoch 274)\n",
      "Doing validation\n",
      "validation loss: 5242544.328\n",
      "2020-02-18 22:07:27.543908: step 481900, loss = 1421355.00 (23.6 examples/sec; 0.679 sec/batch, epoch 275)\n",
      "Doing validation\n",
      "validation loss: 5294018.862\n",
      "2020-02-18 22:08:45.455459: step 482000, loss = 1356778.50 (23.1 examples/sec; 0.692 sec/batch, epoch 276)\n",
      "2020-02-18 22:09:53.989549: step 482100, loss = 1242858.00 (23.1 examples/sec; 0.692 sec/batch, epoch 276)\n",
      "Doing validation\n",
      "validation loss: 5186324.690\n",
      "2020-02-18 22:11:11.857375: step 482200, loss = 1514888.00 (22.9 examples/sec; 0.698 sec/batch, epoch 277)\n",
      "Doing validation\n",
      "validation loss: 5254060.776\n",
      "2020-02-18 22:12:29.814313: step 482300, loss = 1310761.00 (23.3 examples/sec; 0.686 sec/batch, epoch 278)\n",
      "2020-02-18 22:13:38.421556: step 482400, loss = 1248089.00 (23.6 examples/sec; 0.679 sec/batch, epoch 278)\n",
      "Doing validation\n",
      "validation loss: 5241602.931\n",
      "2020-02-18 22:14:56.473240: step 482500, loss = 1225133.50 (23.4 examples/sec; 0.685 sec/batch, epoch 279)\n",
      "Doing validation\n",
      "validation loss: 5229350.121\n",
      "2020-02-18 22:16:14.420174: step 482600, loss = 1342608.50 (23.6 examples/sec; 0.679 sec/batch, epoch 280)\n",
      "2020-02-18 22:17:23.058784: step 482700, loss = 1517333.50 (22.8 examples/sec; 0.701 sec/batch, epoch 280)\n",
      "Doing validation\n",
      "validation loss: 5226895.914\n",
      "2020-02-18 22:18:41.483082: step 482800, loss = 1396930.00 (23.2 examples/sec; 0.688 sec/batch, epoch 281)\n",
      "Doing validation\n",
      "validation loss: 5255932.569\n",
      "2020-02-18 22:19:59.598321: step 482900, loss = 1453020.00 (23.5 examples/sec; 0.680 sec/batch, epoch 282)\n",
      "2020-02-18 22:21:08.379746: step 483000, loss = 1311328.50 (22.8 examples/sec; 0.702 sec/batch, epoch 282)\n",
      "Doing validation\n",
      "validation loss: 5234428.466\n",
      "2020-02-18 22:22:26.502157: step 483100, loss = 1438532.00 (23.4 examples/sec; 0.685 sec/batch, epoch 283)\n",
      "Doing validation\n",
      "validation loss: 5241114.310\n",
      "2020-02-18 22:23:44.634987: step 483200, loss = 1211890.00 (23.1 examples/sec; 0.692 sec/batch, epoch 284)\n",
      "2020-02-18 22:24:53.363256: step 483300, loss = 1101354.50 (23.1 examples/sec; 0.692 sec/batch, epoch 284)\n",
      "Doing validation\n",
      "validation loss: 5226619.000\n",
      "2020-02-18 22:26:12.388901: step 483400, loss = 1524966.50 (23.0 examples/sec; 0.697 sec/batch, epoch 285)\n",
      "Doing validation\n",
      "validation loss: 5303705.190\n",
      "2020-02-18 22:27:30.433595: step 483500, loss = 1189131.00 (23.5 examples/sec; 0.682 sec/batch, epoch 286)\n",
      "2020-02-18 22:28:39.053087: step 483600, loss = 1340976.50 (23.1 examples/sec; 0.693 sec/batch, epoch 286)\n",
      "Doing validation\n",
      "validation loss: 5234883.931\n",
      "2020-02-18 22:29:57.212093: step 483700, loss = 1194409.00 (23.1 examples/sec; 0.693 sec/batch, epoch 287)\n",
      "Doing validation\n",
      "validation loss: 5209715.448\n",
      "2020-02-18 22:31:15.235078: step 483800, loss = 1233884.00 (23.4 examples/sec; 0.684 sec/batch, epoch 288)\n",
      "2020-02-18 22:32:25.383942: step 483900, loss = 1114849.00 (23.2 examples/sec; 0.689 sec/batch, epoch 288)\n",
      "Doing validation\n",
      "validation loss: 5333833.603\n",
      "2020-02-18 22:33:43.327618: step 484000, loss = 1305957.00 (23.1 examples/sec; 0.691 sec/batch, epoch 289)\n",
      "step:  484000 save model:  train484000model.ckpt\n",
      "Doing validation\n",
      "validation loss: 5199617.672\n",
      "2020-02-18 22:35:05.205322: step 484100, loss = 1486271.00 (23.0 examples/sec; 0.695 sec/batch, epoch 290)\n",
      "2020-02-18 22:36:13.744047: step 484200, loss = 1125251.00 (23.6 examples/sec; 0.679 sec/batch, epoch 290)\n",
      "Doing validation\n",
      "validation loss: 5272454.293\n",
      "2020-02-18 22:37:32.596784: step 484300, loss = 1298825.50 (22.4 examples/sec; 0.714 sec/batch, epoch 291)\n",
      "Doing validation\n",
      "validation loss: 5261562.397\n",
      "2020-02-18 22:38:51.625182: step 484400, loss = 1206239.00 (23.3 examples/sec; 0.688 sec/batch, epoch 292)\n"
     ]
    }
   ],
   "source": [
    "print('%s start' % datetime.now())\n",
    "train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
