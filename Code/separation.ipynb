{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-17T12:44:15.014366Z",
     "start_time": "2020-04-17T12:44:08.980939Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\91828\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\91828\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\91828\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\91828\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\91828\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\91828\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "C:\\Users\\91828\\Anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\91828\\Anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\91828\\Anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\91828\\Anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\91828\\Anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\91828\\Anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "from datetime import datetime\n",
    "import os.path\n",
    "import time\n",
    "import numpy as np\n",
    "import librosa\n",
    "import pickle\n",
    "from numpy.lib import stride_tricks\n",
    "import ipdb\n",
    "import os\n",
    "import matplotlib as mpl\n",
    "mpl.use('agg')\n",
    "from matplotlib import pyplot as plt\n",
    "#from GlobalConstont import *\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "import itertools\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from numpy.fft import fft, fftshift\n",
    "import tensorflow as tf\n",
    "from scipy.io import wavfile\n",
    "from winsound import *\n",
    "import soundfile as sf\n",
    "from soundfile import SoundFile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-17T12:45:42.134179Z",
     "start_time": "2020-04-17T12:45:42.128194Z"
    }
   },
   "outputs": [],
   "source": [
    "FRAMES_PER_SAMPLE = 100  # number of frames forming a chunk of data\n",
    "SAMPLING_RATE = 8000\n",
    "FRAME_SIZE = 256\n",
    "NEFF = 129  # effective FFT points\n",
    "# amplification factor of the waveform sig\n",
    "AMP_FAC = 10000\n",
    "MIN_AMP = 10000\n",
    "# TF bins smaller than THRESHOLD will be\n",
    "# considered inactive\n",
    "THRESHOLD = 40\n",
    "# embedding dimention\n",
    "EMBBEDDING_D = 40\n",
    "# prams for pre-whitening\n",
    "GLOBAL_MEAN = 44\n",
    "GLOBAL_STD = 15.5\n",
    "# feed forward dropout prob\n",
    "P_DROPOUT_FF = 0.5\n",
    "# recurrent dropout prob\n",
    "P_DROPOUT_RC = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-17T12:46:11.844795Z",
     "start_time": "2020-04-17T12:46:11.832859Z"
    }
   },
   "outputs": [],
   "source": [
    "def stft(sig, frameSize, overlapFac=0.75, window=np.hanning):\n",
    "    \"\"\" short time fourier transform of audio signal \"\"\"\n",
    "    win = window(frameSize)\n",
    "    hopSize = int(frameSize - np.floor(overlapFac * frameSize))\n",
    "    samples = np.array(sig, dtype='float64')\n",
    "    # cols for windowing\n",
    "    cols = int(np.ceil((len(samples) - frameSize) / float(hopSize)))\n",
    "    frames = stride_tricks.as_strided(\n",
    "        samples,\n",
    "        shape=(cols, frameSize),\n",
    "        strides=(samples.strides[0] * hopSize, samples.strides[0])).copy()\n",
    "    frames *= win\n",
    "    return np.fft.rfft(frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-17T12:47:31.038800Z",
     "start_time": "2020-04-17T12:47:30.998907Z"
    }
   },
   "outputs": [],
   "source": [
    "class Model(object):\n",
    "    def __init__(self, n_hidden, batch_size, p_keep_ff, p_keep_rc):\n",
    "        '''n_hidden: number of hidden states\n",
    "           p_keep_ff: forward keep probability\n",
    "           p_keep_rc: recurrent keep probability'''\n",
    "        self.n_hidden = n_hidden\n",
    "        self.batch_size = batch_size\n",
    "        self.p_keep_ff = 1\n",
    "        self.p_keep_rc = 1\n",
    "        # biases and weights for the last layer\n",
    "        self.weights = {\n",
    "            'out': tf.Variable(\n",
    "                tf.random_normal([2 * n_hidden, EMBBEDDING_D * NEFF]))\n",
    "        }\n",
    "        self.biases = {\n",
    "            'out': tf.Variable(\n",
    "                tf.random_normal([EMBBEDDING_D * NEFF]))\n",
    "        }\n",
    "\n",
    "    def inference(self, x):\n",
    "        '''The structure of the network'''\n",
    "        # four layer of LSTM cell blocks\n",
    "        with tf.variable_scope('BLSTM1') as scope:\n",
    "            lstm_fw_cell = tf.contrib.rnn.LayerNormBasicLSTMCell(\n",
    "                self.n_hidden, layer_norm=False,\n",
    "                dropout_keep_prob=self.p_keep_rc) # by default the activation function is tanh \n",
    "            lstm_fw_cell = tf.nn.rnn_cell.DropoutWrapper(\n",
    "                lstm_fw_cell, input_keep_prob=1,\n",
    "                output_keep_prob=self.p_keep_ff)\n",
    "            lstm_bw_cell = tf.contrib.rnn.LayerNormBasicLSTMCell(\n",
    "                self.n_hidden, layer_norm=False,\n",
    "                dropout_keep_prob=self.p_keep_rc)\n",
    "            lstm_bw_cell = tf.nn.rnn_cell.DropoutWrapper(\n",
    "                lstm_bw_cell, input_keep_prob=1,\n",
    "                output_keep_prob=self.p_keep_ff)\n",
    "            outputs, _ = tf.nn.bidirectional_dynamic_rnn(\n",
    "                lstm_fw_cell, lstm_bw_cell, x,\n",
    "                sequence_length=[FRAMES_PER_SAMPLE] * self.batch_size, \n",
    "                dtype=tf.float32)\n",
    "            state_concate = tf.concat(outputs, 2) \n",
    "        with tf.variable_scope('BLSTM2') as scope:\n",
    "            lstm_fw_cell2 = tf.contrib.rnn.LayerNormBasicLSTMCell(\n",
    "                self.n_hidden, layer_norm=False,\n",
    "                dropout_keep_prob=self.p_keep_rc)\n",
    "            lstm_fw_cell2 = tf.nn.rnn_cell.DropoutWrapper(\n",
    "                lstm_fw_cell2, input_keep_prob=1,\n",
    "                output_keep_prob=self.p_keep_ff)\n",
    "            lstm_bw_cell2 = tf.contrib.rnn.LayerNormBasicLSTMCell(\n",
    "                self.n_hidden, layer_norm=False,\n",
    "                dropout_keep_prob=self.p_keep_rc)\n",
    "            lstm_bw_cell2 = tf.nn.rnn_cell.DropoutWrapper(\n",
    "                lstm_bw_cell2, input_keep_prob=1,\n",
    "                output_keep_prob=self.p_keep_ff)\n",
    "            outputs2, _ = tf.nn.bidirectional_dynamic_rnn(\n",
    "                lstm_fw_cell2, lstm_bw_cell2, state_concate,\n",
    "                sequence_length=[FRAMES_PER_SAMPLE] * self.batch_size,\n",
    "                dtype=tf.float32)\n",
    "            state_concate2 = tf.concat(outputs2, 2)\n",
    "        with tf.variable_scope('BLSTM3') as scope:\n",
    "            lstm_fw_cell3 = tf.contrib.rnn.LayerNormBasicLSTMCell(\n",
    "                self.n_hidden, layer_norm=False,\n",
    "                dropout_keep_prob=self.p_keep_rc)\n",
    "            lstm_fw_cell3 = tf.nn.rnn_cell.DropoutWrapper(\n",
    "                lstm_fw_cell3, input_keep_prob=1,\n",
    "                output_keep_prob=self.p_keep_ff)\n",
    "            lstm_bw_cell3 = tf.contrib.rnn.LayerNormBasicLSTMCell(\n",
    "                self.n_hidden, layer_norm=False,\n",
    "                dropout_keep_prob=self.p_keep_rc)\n",
    "            lstm_bw_cell3 = tf.nn.rnn_cell.DropoutWrapper(\n",
    "                lstm_bw_cell3, input_keep_prob=1,\n",
    "                output_keep_prob=self.p_keep_ff)\n",
    "            outputs3, _ = tf.nn.bidirectional_dynamic_rnn(\n",
    "                lstm_fw_cell3, lstm_bw_cell3, state_concate2,\n",
    "                sequence_length=[FRAMES_PER_SAMPLE] * self.batch_size,\n",
    "                dtype=tf.float32)\n",
    "            state_concate3 = tf.concat(outputs3, 2)\n",
    "        with tf.variable_scope('BLSTM4') as scope:\n",
    "            lstm_fw_cell4 = tf.contrib.rnn.LayerNormBasicLSTMCell(\n",
    "                self.n_hidden, layer_norm=False,\n",
    "                dropout_keep_prob=self.p_keep_rc)\n",
    "            lstm_fw_cell4 = tf.nn.rnn_cell.DropoutWrapper(\n",
    "                lstm_fw_cell4, input_keep_prob=1,\n",
    "                output_keep_prob=self.p_keep_ff)\n",
    "            lstm_bw_cell4 = tf.contrib.rnn.LayerNormBasicLSTMCell(\n",
    "                self.n_hidden, layer_norm=False,\n",
    "                dropout_keep_prob=self.p_keep_rc)\n",
    "            lstm_bw_cell4 = tf.nn.rnn_cell.DropoutWrapper(\n",
    "                lstm_bw_cell4, input_keep_prob=1,\n",
    "                output_keep_prob=self.p_keep_ff)\n",
    "            outputs4, _ = tf.nn.bidirectional_dynamic_rnn(\n",
    "                lstm_fw_cell4, lstm_bw_cell4, state_concate3,\n",
    "                sequence_length=[FRAMES_PER_SAMPLE] * self.batch_size,\n",
    "                dtype=tf.float32)\n",
    "            state_concate4 = tf.concat(outputs4, 2)\n",
    "        # one layer of embedding output with tanh activation function\n",
    "        out_concate = tf.reshape(state_concate4, [-1, self.n_hidden * 2])\n",
    "        emb_out = tf.matmul(out_concate,\n",
    "                            self.weights['out']) + self.biases['out']\n",
    "        emb_out = tf.nn.tanh(emb_out)\n",
    "        reshaped_emb = tf.reshape(emb_out, [-1, NEFF, EMBBEDDING_D])\n",
    "        # normalization before output\n",
    "        normalized_emb = tf.nn.l2_normalize(reshaped_emb, 2)\n",
    "        return normalized_emb\n",
    "\n",
    "    def loss(self, embeddings, Y, VAD):\n",
    "        '''Defining the loss function'''\n",
    "        embeddings_rs = tf.reshape(embeddings, shape=[-1, EMBBEDDING_D])\n",
    "        VAD_rs = tf.reshape(VAD, shape=[-1])\n",
    "        # get the embeddings with active VAD\n",
    "        embeddings_rsv = tf.transpose(\n",
    "            tf.multiply(tf.transpose(embeddings_rs), VAD_rs))\n",
    "        embeddings_v = tf.reshape(\n",
    "            embeddings_rsv, [-1, FRAMES_PER_SAMPLE * NEFF, EMBBEDDING_D])\n",
    "        # get the Y(speaker indicator function) with active VAD\n",
    "        Y_rs = tf.reshape(Y, shape=[-1, 2])\n",
    "        Y_rsv = tf.transpose(\n",
    "            tf.multiply(tf.transpose(Y_rs), VAD_rs))\n",
    "        Y_v = tf.reshape(Y_rsv, shape=[-1, FRAMES_PER_SAMPLE * NEFF, 2])\n",
    "        # fast computation format of the embedding loss function\n",
    "        loss_batch = tf.nn.l2_loss(tf.matmul(tf.transpose(embeddings_v, [0, 2, 1]), embeddings_v)) - \\\n",
    "            2 * tf.nn.l2_loss(tf.matmul(tf.transpose(embeddings_v, [0, 2, 1]), Y_v)) + \\\n",
    "            tf.nn.l2_loss(tf.matmul(tf.transpose(Y_v, [0, 2, 1]), Y_v))\n",
    "        \n",
    "        loss_v = (loss_batch) / self.batch_size\n",
    "        tf.summary.scalar('loss', loss_v)\n",
    "        return loss_v\n",
    "\n",
    "    def train(self, loss, lr):\n",
    "        '''Optimizer'''\n",
    "        optimizer = tf.train.AdamOptimizer(learning_rate=lr,beta1=0.9,beta2=0.999,epsilon=1e-8)\n",
    "        # optimizer = tf.train.MomentumOptimizer(lr, 0.9)\n",
    "        gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "        gradients, _ = tf.clip_by_global_norm(gradients, 200)\n",
    "        train_op = optimizer.apply_gradients(zip(gradients, v))\n",
    "        return train_op\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-17T12:47:37.958166Z",
     "start_time": "2020-04-17T12:47:37.942209Z"
    }
   },
   "outputs": [],
   "source": [
    "class AudioSampleReader(object):\n",
    "    '''\n",
    "    Class AudioSampleReader:\n",
    "        read in a single audio sample for test using trained model\n",
    "    '''\n",
    "    def __init__(self, data_dir):\n",
    "        '''Load in the audio file and transform the signal into\n",
    "        the formats required by the model'''\n",
    "        # loading and transformation\n",
    "        speech_mix, _ = librosa.load(data_dir, SAMPLING_RATE)\n",
    "        speech_mix_spec0 = stft(speech_mix, FRAME_SIZE)[:, :NEFF]\n",
    "        speech_mix_spec = np.abs(speech_mix_spec0)\n",
    "        speech_phase = speech_mix_spec0 / speech_mix_spec\n",
    "        speech_mix_spec = np.maximum(\n",
    "            speech_mix_spec, np.max(speech_mix_spec) / MIN_AMP)\n",
    "        speech_mix_spec = 20. * np.log10(speech_mix_spec * AMP_FAC)\n",
    "        max_mag = np.max(speech_mix_spec)\n",
    "        speech_VAD = (speech_mix_spec > (max_mag - THRESHOLD)).astype(int)\n",
    "        speech_mix_spec = (speech_mix_spec - GLOBAL_MEAN) / GLOBAL_STD\n",
    "        len_spec = speech_mix_spec.shape[0]\n",
    "        k = 0\n",
    "        self.ind = 0\n",
    "        self.samples = []\n",
    "        # feed the transformed data into a sample list\n",
    "        while(k + FRAMES_PER_SAMPLE < len_spec):\n",
    "            phase = speech_phase[k: k + FRAMES_PER_SAMPLE, :]\n",
    "            sample_mix = speech_mix_spec[k:k + FRAMES_PER_SAMPLE, :]\n",
    "            VAD = speech_VAD[k:k + FRAMES_PER_SAMPLE, :]\n",
    "            sample_dict = {'Sample': sample_mix,\n",
    "                           'VAD': VAD,\n",
    "                           'Phase': phase}\n",
    "            self.samples.append(sample_dict)\n",
    "            k = k + FRAMES_PER_SAMPLE\n",
    "        # import ipdb; ipdb.set_trace()\n",
    "        n_left = FRAMES_PER_SAMPLE - len_spec + k\n",
    "        # store phase for waveform reconstruction\n",
    "        phase = np.concatenate((speech_phase[k:, :], np.zeros([n_left, NEFF])))\n",
    "        sample_mix = np.concatenate(\n",
    "            (speech_mix_spec[k:, :], np.zeros([n_left, NEFF])))\n",
    "        VAD = np.concatenate((speech_VAD[k:, :], np.zeros([n_left, NEFF])))\n",
    "        sample_dict = {'Sample': sample_mix,\n",
    "                       'VAD': VAD,\n",
    "                       'Phase': phase}\n",
    "        self.samples.append(sample_dict)\n",
    "        self.tot_samp = len(self.samples)\n",
    "\n",
    "    def gen_next(self):\n",
    "        # ipdb.set_trace()\n",
    "        begin = self.ind\n",
    "        if begin >= self.tot_samp:\n",
    "            return None\n",
    "        self.ind += 1\n",
    "        return [self.samples[begin]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-17T12:48:00.889809Z",
     "start_time": "2020-04-17T12:48:00.883828Z"
    }
   },
   "outputs": [],
   "source": [
    "# For separation\n",
    "lr = 0.00001  # not useful during test\n",
    "n_hidden = 300  # hidden state size\n",
    "batch_size = 1  # 1 for audio sample test\n",
    "hop_size = 64\n",
    "# oracle flag to decide if a frame need to be seperated\n",
    "sep_flag = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1] * 10\n",
    "# oracle permutation to concatenate the chuncks of output frames\n",
    "oracal_p = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] * 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-17T12:48:16.969194Z",
     "start_time": "2020-04-17T12:48:16.923317Z"
    }
   },
   "outputs": [],
   "source": [
    "def out_put(N_frame):\n",
    "    '''Use trained model to infer N _frame chuncks of\n",
    "    frames of input audio'''\n",
    "    with tf.Graph().as_default():\n",
    "        # feed forward keep prob\n",
    "        p_keep_ff = tf.placeholder(tf.float32, shape=None)\n",
    "        # recurrent keep prob\n",
    "        p_keep_rc = tf.placeholder(tf.float32, shape=None)\n",
    "        # audio sample generator\n",
    "        data_generator = AudioSampleReader(\"demo.wav\")\n",
    "        \n",
    "        # placeholder for model input\n",
    "        in_data = tf.placeholder(\n",
    "            tf.float32, shape=[batch_size, FRAMES_PER_SAMPLE, NEFF])\n",
    "        # init the model\n",
    "        BiModel = Model(n_hidden, batch_size, p_keep_ff, p_keep_rc)\n",
    "        # make inference of embedding\n",
    "        embedding = BiModel.inference(in_data)\n",
    "        saver = tf.train.Saver(tf.all_variables())\n",
    "        sess = tf.Session()\n",
    "        # restore the model\n",
    "        saver.restore(sess, 'train/trained/model.ckpt')\n",
    "        tot_frame = N_frame * FRAMES_PER_SAMPLE\n",
    "        # arrays to store output waveform\n",
    "        out_audio1 = np.zeros([(tot_frame - 1) * hop_size + FRAME_SIZE])\n",
    "        out_audio2 = np.zeros([(tot_frame - 1) * hop_size + FRAME_SIZE])\n",
    "        mix = np.zeros([(tot_frame - 1) * hop_size + FRAME_SIZE])\n",
    "        N_assign = 0\n",
    "\n",
    "        # for every chunk of frames of data\n",
    "        for step in range(N_frame):\n",
    "            # import ipdb; ipdb.set_trace()\n",
    "            data_batch = data_generator.gen_next()\n",
    "            if data_batch is None:\n",
    "                break\n",
    "            # log spectrum info.\n",
    "            in_data_np = np.concatenate(\n",
    "                [np.reshape(item['Sample'], [1, FRAMES_PER_SAMPLE, NEFF])\n",
    "                 for item in data_batch])\n",
    "            # phase info.\n",
    "            in_phase_np = np.concatenate(\n",
    "                [np.reshape(item['Phase'], [1, FRAMES_PER_SAMPLE, NEFF])\n",
    "                 for item in data_batch])\n",
    "            # VAD info.\n",
    "            VAD_data_np = np.concatenate(\n",
    "                [np.reshape(item['VAD'], [1, FRAMES_PER_SAMPLE, NEFF])\n",
    "                 for item in data_batch])\n",
    "\n",
    "            # get inferred embedding using trained model\n",
    "            # with keep prob = 1\n",
    "            embedding_np, = sess.run(\n",
    "                [embedding],\n",
    "                feed_dict={in_data: in_data_np,\n",
    "                           p_keep_ff: 1,\n",
    "                           p_keep_rc: 1})\n",
    "            # ipdb.set_trace()\n",
    "            # get active TF-bin embedding according to VAD\n",
    "            embedding_ac = [embedding_np[i, j, :]\n",
    "                            for i, j in itertools.product(\n",
    "                                range(FRAMES_PER_SAMPLE), range(NEFF))\n",
    "                            if VAD_data_np[0, i, j] == 1]\n",
    "            if(sep_flag[step] == 1):\n",
    "                # if the frame need to be seperated\n",
    "                # cluster the embeddings\n",
    "                # import ipdb; ipdb.set_trace()\n",
    "                if embedding_ac == []:\n",
    "                    break\n",
    "                kmean = KMeans(n_clusters=2, random_state=0).fit(embedding_ac)\n",
    "\n",
    "            else:\n",
    "                # if the frame don't need to be seperated\n",
    "                # don't split the embeddings\n",
    "                kmean = KMeans(n_clusters=1, random_state=0).fit(embedding_ac)\n",
    "            mask = np.zeros([FRAMES_PER_SAMPLE, NEFF, 2])\n",
    "            ind = 0\n",
    "            if N_assign == 0:\n",
    "                # if their is no existing speaker in previous frame\n",
    "                center = kmean.cluster_centers_\n",
    "                N_assign = center.shape[0]\n",
    "            elif N_assign == 1:\n",
    "                # if their is one speaker in previous frame\n",
    "                center_new = kmean.cluster_centers_\n",
    "                # assign the embedding for a speaker to the speaker with the\n",
    "                # closest centroid in previous frames\n",
    "                if center_new.shape[0] == 1:\n",
    "                    # update and smooth the centroid for 1 speaker\n",
    "                    center = 0.7 * center + 0.3 * center_new\n",
    "                else:\n",
    "                    # update and smooth the centroid for 2 speakers\n",
    "                    N_assign = 2\n",
    "                    # compute their relative affinity\n",
    "                    cor = np.matmul(center_new, np.transpose(center))\n",
    "                    # ipdb.set_trace()\n",
    "                    if(cor[1] > cor[0]):\n",
    "                        # rearrange their sequence if not consistant with\n",
    "                        # previous frames\n",
    "                        kmean.cluster_centers_ = np.array(\n",
    "                            [kmean.cluster_centers_[1],\n",
    "                             kmean.cluster_centers_[0]])\n",
    "                        kmean.labels_ = (kmean.labels_ == 0).astype('int')\n",
    "                    center = kmean.cluster_centers_\n",
    "            else:\n",
    "                # two speakers have appeared\n",
    "                center_new = kmean.cluster_centers_\n",
    "                cor = np.matmul(center_new[0, :], np.transpose(center))\n",
    "                # rearrange their sequence if not consistant with previous\n",
    "                # frames\n",
    "                if(cor[1] > cor[0]):\n",
    "                    if(sep_flag[step] == 1):\n",
    "                        kmean.cluster_centers_ = np.array(\n",
    "                            [kmean.cluster_centers_[1],\n",
    "                             kmean.cluster_centers_[0]])\n",
    "                        kmean.labels_ = (kmean.labels_ == 0).astype('int')\n",
    "                    else:\n",
    "                        kmean.labels_ = (kmean.labels_ == 1).astype('int')\n",
    "                # need permutation of their order(Oracle)\n",
    "                if(oracal_p[step]):\n",
    "                    kmean.cluster_centers_ = np.array(\n",
    "                        [kmean.cluster_centers_[1],\n",
    "                         kmean.cluster_centers_[0]])\n",
    "                    kmean.labels_ = (kmean.labels_ == 0).astype('int')\n",
    "                else:\n",
    "                    kmean.labels_ = (~kmean.labels_).astype('int')\n",
    "                center = center * 0.7 + 0.3 * kmean.cluster_centers_\n",
    "\n",
    "            # transform the clustering result and VAD info. into masks\n",
    "            for i in range(FRAMES_PER_SAMPLE):\n",
    "                for j in range(NEFF):\n",
    "                    if VAD_data_np[0, i, j] == 1:\n",
    "                        mask[i, j, kmean.labels_[ind]] = 1\n",
    "                        ind += 1\n",
    "            for i in range(FRAMES_PER_SAMPLE):\n",
    "                # apply the mask and reconstruct the waveform\n",
    "                tot_ind = step * FRAMES_PER_SAMPLE + i\n",
    "                # ipdb.set_trace()\n",
    "                # amp = (in_data_np[0, i, :] *\n",
    "                #        data_batch[0]['Std']) + data_batch[0]['Mean']\n",
    "                amp = in_data_np[0, i, :] * GLOBAL_STD + GLOBAL_MEAN\n",
    "                out_data1 = (mask[i, :, 0] * amp *\n",
    "                             VAD_data_np[0, i, :])\n",
    "                out_data2 = (mask[i, :, 1] * amp *\n",
    "                             VAD_data_np[0, i, :])\n",
    "                out_mix = amp\n",
    "                out_data1_l = 10 ** (out_data1 / 20) / AMP_FAC\n",
    "                out_data2_l = 10 ** (out_data2 / 20) / AMP_FAC\n",
    "                out_mix_l = 10 ** (out_mix / 20) / AMP_FAC\n",
    "\n",
    "                out_stft1 = out_data1_l * in_phase_np[0, i, :]\n",
    "                out_stft2 = out_data2_l * in_phase_np[0, i, :]\n",
    "                out_stft_mix = out_mix_l * in_phase_np[0, i, :]\n",
    "\n",
    "                con_data1 = out_stft1[-2:0:-1].conjugate()\n",
    "                con_data2 = out_stft2[-2:0:-1].conjugate()\n",
    "                con_mix = out_stft_mix[-2:0:-1].conjugate()\n",
    "\n",
    "                out1 = np.concatenate((out_stft1, con_data1))\n",
    "                out2 = np.concatenate((out_stft2, con_data2))\n",
    "                out_mix = np.concatenate((out_stft_mix, con_mix))\n",
    "                frame_out1 = np.fft.ifft(out1).astype(np.float64)\n",
    "                frame_out2 = np.fft.ifft(out2).astype(np.float64)\n",
    "                frame_mix = np.fft.ifft(out_mix).astype(np.float64)\n",
    "\n",
    "                out_audio1[tot_ind * hop_size:tot_ind * hop_size + FRAME_SIZE] += frame_out1 * 0.5016\n",
    "                out_audio2[tot_ind * hop_size:tot_ind * hop_size + FRAME_SIZE] += frame_out2 * 0.5016\n",
    "                mix[tot_ind * hop_size:tot_ind * hop_size + FRAME_SIZE] += frame_mix * 0.5016\n",
    "\n",
    "        #librosa.output.write_wav('out_srt01.wav', out_audio1, SAMPLING_RATE)\n",
    "        #librosa.output.write_wav('out_srt02.wav', out_audio2, SAMPLING_RATE)\n",
    "        sf.write('out_demo01.wav', out_audio1, SAMPLING_RATE, 'PCM_24')\n",
    "        sf.write('out_demo02.wav', out_audio2, SAMPLING_RATE, 'PCM_24')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-15T16:16:28.807916Z",
     "start_time": "2020-04-15T16:16:16.716317Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\91828\\AppData\\Roaming\\Python\\Python37\\site-packages\\librosa\\core\\audio.py:161: UserWarning: PySoundFile failed. Trying audioread instead.\n",
      "  warnings.warn('PySoundFile failed. Trying audioread instead.')\n",
      "C:\\Users\\91828\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:13: RuntimeWarning: invalid value encountered in true_divide\n",
      "  del sys.path[0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from train/trained/model.ckpt\n"
     ]
    }
   ],
   "source": [
    "out_put(13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
